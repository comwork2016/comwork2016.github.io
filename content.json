{"meta":{"title":"Liao's Blog","subtitle":"强迫症患者","description":null,"author":"huangyedi2012","url":"https://comwork2016.github.io"},"pages":[{"title":"分类","date":"2017-03-31T05:54:13.000Z","updated":"2017-04-01T11:56:30.619Z","comments":true,"path":"categories/index.html","permalink":"https://comwork2016.github.io/categories/index.html","excerpt":"","text":"","raw":null,"content":null},{"title":"关于","date":"2017-03-31T05:22:46.000Z","updated":"2017-04-01T11:56:30.619Z","comments":true,"path":"about/index.html","permalink":"https://comwork2016.github.io/about/index.html","excerpt":"","text":"本博客中的内容是本人在计算机专业学习中所作的记录，只是方便本人在以后有所遗忘时进行查阅。有部分文章是从网络中获取，如有侵权，请联系我。","raw":null,"content":null},{"title":"标签","date":"2017-03-31T05:54:06.000Z","updated":"2017-04-01T11:56:30.626Z","comments":true,"path":"tags/index.html","permalink":"https://comwork2016.github.io/tags/index.html","excerpt":"","text":"","raw":null,"content":null}],"posts":[{"title":"Spark On Yarn 集群安装部署","slug":"Spark-On-Yarn-集群安装部署","date":"2017-06-05T07:24:19.000Z","updated":"2017-06-05T08:16:56.681Z","comments":true,"path":"2017/06/05/Spark-On-Yarn-集群安装部署/","link":"","permalink":"https://comwork2016.github.io/2017/06/05/Spark-On-Yarn-集群安装部署/","excerpt":"本文记录的是Spark在Yarn上的集群安装部署","text":"本文记录的是Spark在Yarn上的集群安装部署 安装环境OS: CentOS release 6.7 (Final)Hadoop: 2.7.3Spark: spark-2.1.1-bin-hadoop2.7 先决条件安装 Scala从官方下载地址下载scala。 修改环境变量sudo vi /etc/profile，添加以下内容： 12export SCALA_HOME=$HOME/local/opt/scala-2.12.2export PATH=$PATH:$SCALA_HOME/bin 验证 scala 是否安装成功： 1scala -version #如果打印出如下版本信息，则说明安装成功 安装配置 Hadoop YarnHadoop Yarn的安装见Hadoop 2.7.3 安装 Spark安装下载解压进入官方下载地址下载最新版Spark。 在~/local/opt目录下解压 1tar xzvf spark-2.1.1-bin-hadoop2.7.tgz 配置 Spark123cd ~/local/opt/spark-2.1.1-bin-hadoop2.7/conf #进入spark配置目录cp spark-env.sh.template spark-env.sh #从配置模板复制vi spark-env.sh #添加配置内容 在spark-env.sh末尾添加以下内容: 1234567export SCALA_HOME=/home/hadoop/local/opt/scala-2.12.2export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk.x86_64export HADOOP_HOME=/home/hadoop/local/opt/hadoop-2.7.3export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopSPARK_MASTER_IP=masterSPARK_LOCAL_DIRS=/home/hadoop/local/opt/spark-2.1.1-bin-hadoop2.7SPARK_DRIVER_MEMORY=1G 在slaves文件下填上slave主机名： 12slave1slave2 将配置好的scala-2.12.2和spark-2.1.1-bin-hadoop2.7文件夹分发给所有slaves: 1234scp -r ~/local/opt/scala-2.12.2/ hadoop@slave1:~/local/opt/scp -r ~/local/opt/scala-2.12.2/ hadoop@slave2:~/local/opt/scp -r ~/local/opt/spark-2.1.1-bin-hadoop2.7 hadoop@slave1:~/local/opt/scp -r ~/local/opt/spark-2.1.1-bin-hadoop2.7 hadoop@slave2:~/local/opt/ 启动Spark1sbin/start-all.sh 验证Spark是否安装成功用jps检查，在 master 上应该有以下几个进程： 123456786484 SecondaryNameNode9156 HQuorumPeer9223 HMaster24871 JobHistoryServer19771 Master6283 NameNode6653 ResourceManager20222 Jps 在 slave 上应该有以下几个进程： 12345617088 NodeManager17779 HRegionServer31145 Worker17692 HQuorumPeer16973 DataNode31390 Jps 进入Spark的Web管理页面： http://master:8080 参考文献 Spark On YARN 集群安装部署","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"https://comwork2016.github.io/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://comwork2016.github.io/tags/spark/"}]},{"title":"神经网络 Neural Networks","slug":"神经网络-Neural-Networks","date":"2017-04-27T02:35:44.000Z","updated":"2017-05-18T03:43:57.850Z","comments":true,"path":"2017/04/27/神经网络-Neural-Networks/","link":"","permalink":"https://comwork2016.github.io/2017/04/27/神经网络-Neural-Networks/","excerpt":"人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。","text":"人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。 神经元 这个“神经元”是一个以 $x_1,x_2,x_3$ 及截距 $ +1 $ 为输入值的运算单元，其输出为 $ h_{W,b}(x) = f(W^Tx) = f(\\sum_{i=1}^3 W_{i}x_i +b)$ ，其中函数 $ f : \\Re \\mapsto \\Re$ 被称为“激活函数”。在本教程中，我们选用sigmoid函数作为激活函数 $ f(\\cdot) $:$$f(z) = \\frac{1}{1+\\exp(-z)}.$$ 神经网络模型所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。例如，下图就是一个简单的神经网络： 我们使用 $ w^l_{jk} $ 表示从 $(l−1)^{th}$ 层的 $k^{th} $个神经元到 $l^{th} $ 层的 $j^{th} $ 个神经元的链接上的权重。使用 $b^l_j$ 表示在 $l^{th}$ 层第 $j^{th} $ 个神经元的偏置，中间量 $ z^l \\equiv w^l a^{l-1}+b^l$ ，使用 $a^l_j$ 表示 $l^{th}$ 层第 $j^{th}$ 个神经元的激活值。 $l^{th}$ 层的第 $j^{th}$ 个神经元的激活值 $a^l_j$ 就和 $l-1^{th}$ 层的激活值通过方程关联起来了。 $$\\begin{eqnarray}a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right)\\label{eq:fp}\\tag{fp}\\end{eqnarray}$$ 对方程$\\eqref{eq:fp}$ 就可以写成下面这种美妙而简洁的向量形式了 $$\\begin{eqnarray} a^{l} = \\sigma(w^l a^{l-1}+b^l) \\label{eq:mfp}\\tag{mfp}\\end{eqnarray}$$ 反向传播反向传播的目标是计算代价函数 $C$ 分别关于 $w$ 和 $b$ 的偏导数 $∂C/∂w$ 和 $∂C/∂b$ 。反向传播其实是对权重和偏置变化影响代价函数过程的理解。最终极的含义其实就是计算偏导数 $\\partial C/\\partial w_{jk}^l$ 和$\\partial C/\\partial b_j^l$。但是为了计算这些值，我们首先引入一个中间量， $\\delta_j^l$ ，这个我们称为在 $l^{th}$ 层第 $j^{th}$ 个神经元上的误差。 对于$l$层的第 $j^{th}$ 个神经元，当输入进来时，对神经元的带权输入增加很小的变化 $\\Delta z_j^l$ ，使得神经元输出由 $\\sigma(z_j^l)$ 变成 $\\sigma(z_j^l + \\Delta z_j^l)$ 。这个变化会向网络后面的层进行传播，最终导致整个代价产生 $\\frac{\\partial C}{\\partial z_j^l} \\Delta z_j^l$ 的改变。所以这里有一种启发式的认识， $\\frac{\\partial C}{\\partial z_j^l}$ 是神经元的误差的度量。 按照上面的描述，我们定义 $l$ 层的第 $j^{th}$ 个神经元上的误差 $\\delta_j^l$ 为：$$\\begin{eqnarray} \\delta^l_j \\equiv \\frac{\\partial C}{\\partial z^l_j} \\label{eq:error}\\tag{error}\\end{eqnarray}$$ 输出层误差的方程输出层误差的方程， $\\delta^L$ ： 每个元素定义如下： $$\\begin{eqnarray} \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma’(z^L_j) \\label{eq:bp1}\\tag{BP1}\\end{eqnarray}$$ 第一个项 $\\partial C/\\partial a_j^L$ 表示代价随着 $j^{th}$ 输出激活值的变化而变化的速度。第二项 $\\sigma’(z^L_j)$ 刻画了在 $z_j^L$ 处激活函数 $\\sigma$ 变化的速度。 使用下一层的误差 $\\delta^{l+1}$ 来表示当前层的误差 $\\delta^{l}$使用下一层的误差 $\\delta^{l+1}$ 来表示当前层的误差 $\\delta^{l}$：特别地， $$\\begin{eqnarray} \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma’(z^l) \\label{eq:bp2}\\tag{BP2}\\end{eqnarray}$$ 其中$(w^{l+1})^T$是$(l+1)^{\\rm th}$层权重矩阵$w^{l+1}$的转置。假设我们知道$l+1^{\\rm th}$层的误差$\\delta^{l+1}$。当我们应用转置的权重矩阵$(w^{l+1})^T$，我们可以凭直觉地把它看作是在沿着网络反向移动误差，给了我们度量在$l^{\\rm th}$ 层输出的误差方法。然后，我们进行 Hadamard 乘积运算 $\\odot \\sigma’(z^l)$ 。这会让误差通过 $l$ 层的激活函数反向传递回来并给出在第 $l$ 层的带权输入的误差 $\\delta$ 。 证明：我们想要以$\\delta^{l+1}_k = \\partial C / \\partial z^{l+1}_k$的形式重写$\\delta^l_j = \\partial C / \\partial z^l_j$。应用链式法则$$\\begin{eqnarray}\\delta^l_j &amp;=&amp; \\frac{\\partial C}{\\partial z^l_j}\\\\ &amp;=&amp; \\sum_k \\frac{\\partial C}{\\partial z^{l+1}_k} \\frac{\\partial z^{l+1}_k}{\\partial z^l_j}\\\\ &amp;=&amp; \\sum_k \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} \\delta^{l+1}_k\\end{eqnarray}$$ 为了对最后一行的第一项求值，注意： $$\\begin{eqnarray} z^{l+1}_k = \\sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \\sum_j w^{l+1}_{kj} \\sigma(z^l_j) +b^{l+1}_k\\end{eqnarray}$$ 做微分，我们得到 $$\\begin{eqnarray} \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = w^{l+1}_{kj} \\sigma’(z^l_j)\\end{eqnarray}$$ 代入上式即有： $$\\begin{eqnarray} \\delta^l_j = \\sum_k w^{l+1}_{kj} \\delta^{l+1}_k \\sigma’(z^l_j)\\end{eqnarray}$$ 代价函数关于网络中任意偏置的改变率代价函数关于网络中任意偏置的改变率： 就是$$\\begin{eqnarray} \\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j \\label{eq:bp3}\\tag{BP3}\\end{eqnarray}$$ 这其实是，误差$\\delta^l_j$ 和偏导数值 $\\partial C / \\partial b^l_j$完全一致。 代价函数关于任何一个权重的改变率代价函数关于任何一个权重的改变率： 特别地， $$\\begin{eqnarray} \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j \\label{eq:bp4}\\tag{BP4}\\end{eqnarray}$$ 反向传播算法描述 输入$x$： 为输入层设置对应的激活值$a^1$ 前向传播： 对每个$l=2,3,…,L$计算相应的$z^l = w^la^{l-1} + b^l$ 和 $a^l = \\sigma(z^l)$ 输出层误差 $\\delta^L$ ： 计算向量 $\\delta^L = \\nabla_a C \\odot \\sigma’(z^L)$ 反向误差传播： 对每个$l=L-1, L-2,…,2$ ，计算$\\delta^l = ((w^{l+1})^T\\delta^{l+1})\\odot \\sigma’(z^l)$ 输出： 代价函数的梯度由 $\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$ 和 $\\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l$ 得出 证明见四个基本方程的证明。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126import randomimport numpy as npclass Network(object): def __init__(self, sizes): \"\"\"The list ``sizes`` contains the number of neurons in the respective layers of the network. For example, if the list was [2, 3, 1] then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron. The biases and weights for the network are initialized randomly, using a Gaussian distribution with mean 0, and variance 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers.\"\"\" self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] def feedforward(self, a): \"\"\"Return the output of the network if ``a`` is input.\"\"\" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): \"\"\"Train the neural network using mini-batch stochastic gradient descent. The ``training_data`` is a list of tuples ``(x, y)`` representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory. If ``test_data`` is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. This is useful for tracking progress, but slows things down substantially.\"\"\" if test_data: n_test = len(test_data) n = len(training_data) for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print \"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;\".format( j, self.evaluate(test_data), n_test) else: print \"Epoch &#123;0&#125; complete\".format(j) def update_mini_batch(self, mini_batch, eta): \"\"\"Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta`` is the learning rate.\"\"\" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` and ``self.weights``.\"\"\" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # feedforward activation = x activations = [x] # list to store all the activations, layer by layer zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = self.cost_derivative(activations[-1], y) * \\ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It's a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def evaluate(self, test_data): \"\"\"Return the number of test inputs for which the neural network outputs the correct result. Note that the neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation.\"\"\" test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum(int(x == y) for (x, y) in test_results) def cost_derivative(self, output_activations, y): \"\"\"Return the vector of partial derivatives \\partial C_x / \\partial a for the output activations.\"\"\" return (output_activations-y)def sigmoid(z): \"\"\"The sigmoid function.\"\"\" return 1.0/(1.0+np.exp(-z))def sigmoid_prime(z): \"\"\"Derivative of the sigmoid function.\"\"\" return sigmoid(z)*(1-sigmoid(z)) 参考文献 神经网络与深度学习","raw":null,"content":null,"categories":[{"name":"ML","slug":"ML","permalink":"https://comwork2016.github.io/categories/ML/"}],"tags":[{"name":"NN","slug":"NN","permalink":"https://comwork2016.github.io/tags/NN/"},{"name":"BP","slug":"BP","permalink":"https://comwork2016.github.io/tags/BP/"}]},{"title":"Hadoop之Yarn原理","slug":"Hadoop之Yarn原理","date":"2017-04-17T11:43:49.000Z","updated":"2017-04-18T01:58:48.976Z","comments":true,"path":"2017/04/17/Hadoop之Yarn原理/","link":"","permalink":"https://comwork2016.github.io/2017/04/17/Hadoop之Yarn原理/","excerpt":"YARN（Yet Another Resource Negotiator）是一个通用的资源管理平台，可为各类计算框架提供资源的管理和调度。","text":"YARN（Yet Another Resource Negotiator）是一个通用的资源管理平台，可为各类计算框架提供资源的管理和调度。 Hadoop MapReduce 框架的问题随着分布式系统集群的规模和其工作负荷的增长，MapReduce 框架的问题逐渐浮出水面，主要的问题集中如下： JobTracker 是 Map-reduce 的集中处理点，存在单点故障。 JobTracker 完成了太多的任务，造成了过多的资源消耗，当 map-reduce job 非常多的时候，会造成很大的内存开销，潜在来说，也增加了 JobTracker fail 的风险，这也是业界普遍总结出老 Hadoop 的 Map-Reduce 只能支持 4000 节点主机的上限。 在 TaskTracker 端，以 map/reduce task 的数目作为资源的表示过于简单，没有考虑到 cpu/ 内存的占用情况，如果两个大内存消耗的 task 被调度到了一块，很容易出现 OOM(Out-Of-Memory)。 在 TaskTracker 端，把资源强制划分为 map task slot 和 reduce task slot, 如果当系统中只有 map task 或者只有 reduce task 的时候，会造成资源的浪费，也就是前面提过的集群资源利用的问题。 源代码层面分析的时候，会发现代码非常的难读，常常因为一个 class 做了太多的事情，代码量达 3000 多行，造成 class 的任务不清晰，增加 bug 修复和版本维护的难度。 从操作的角度来看，现在的 Hadoop MapReduce 框架在有任何重要的或者不重要的变化 ( 例如 bug 修复，性能提升和特性化 ) 时，都会强制进行系统级别的升级更新。更糟的是，它不管用户的喜好，强制让分布式集群系统的每一个用户端同时更新。这些更新会让用户为了验证他们之前的应用程序是不是适用新的 Hadoop 版本而浪费大量时间。 Yarn介绍Yarn的核心出发点是为了分离资源管理与作业调度/监控，实现分离的做法是拥有一个全局的资源管理器（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM），应用程序由一个作业（Job）或者Job的有向无环图（DAG）组成。 YARN可以将多种计算框架(如离线处理MapReduce、在线处理的Storm、迭代式计算框架Spark、流式处理框架S4等) 部署到一个公共集群中，共享集群的资源。并提供如下功能： 资源的统一管理和调度： 集群中所有节点的资源(内存、CPU、磁盘、网络等)抽象为Container。计算框架需要资源进行运算任务时需要向YARN申请Container， YARN按照特定的策略对资源进行调度进行Container的分配。 资源隔离： YARN使用了轻量级资源隔离机制Cgroups进行资源隔离以避免相互干扰，一旦Container使用的资源量超过事先定义的上限值，就将其杀死。 YARN是对Mapreduce V1重构得到的，有时候也成为MapReduce V2。 YARN可以看成一个云操作系统，由一个ResourceManager和多个NodeManager组成， 它负责管理所有NodeManger上多维度资源， 并以Container(启动一个Container相当于启动一个进程)方式分配给应用程序启动ApplicationMaster(相当于主进程中运行逻辑) 或运行ApplicationMaster切分的各Task(相当于子进程中运行逻辑)。 YARN体系架构YARN架构如下图所示： YARN总体上是Master/Slave结构，主要由ResourceManager、NodeManager、 ApplicationMaster和Container等几个组件构成。 ResourceManager(RM) 负责对各NodeManager上的资源进行统一管理和调度。将ApplicationMaster分配空闲的Container运行并监控其运行状态。对ApplicationMaster申请的资源请求分配相应的空闲Container。主要由两个组件构成：调度器和应用程序管理器： 调度器(Scheduler)：调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位是Container，从而限定每个任务使用的资源量。Scheduler不负责监控或者跟踪应用程序的状态，也不负责任务因为各种原因而需要的重启（由ApplicationMaster负责）。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为应用程序分配封装在Container中的资源。 应用程序管理器(Applications Manager)：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等，跟踪分给的Container的进度、状态也是其职责。 NodeManager (NM) NodeManager是每个节点上的资源和任务管理器。它会定时地向ResourceManager汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自ApplicationMaster的Container 启动/停止等请求。 ApplicationMaster (AM) 用户提交的应用程序均包含一个ApplicationMaster，负责应用的监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster是应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控。MapReduce就是原生支持的一种框架，可以在YARN上运行Mapreduce作业。有很多分布式应用都开发了对应的应用程序框架，用于在YARN上运行任务，例如Spark，Storm等。 Container Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当ApplicationMaster向ResourceManager申请资源时，ResourceManager为ApplicationMaster返回的资源便是用Container 表示的。 YARN会为每个任务分配一个Container且该任务只能使用该Container中描述的资源。 YARN应用工作流程如下图所示用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序： 启动ApplicationMaster，如下步骤1~3； 由ApplicationMaster创建应用程序为它申请资源并监控它的整个运行过程，直到运行完成，如下步骤4~7。 用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、命令参数、用户程序等；事实上，需要准确描述运行ApplicationMaster的unix进程的所有信息。提交工作通常由YarnClient来完成。 ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动ApplicationMaster； ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager査看应用程序的运行状态，运行状态通过 AMRMClientAsync.CallbackHandler的getProgress() 方法来传递给ResourceManager。 然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4〜7； ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源；资源的协调通过AMRMClientAsync异步完成,相应的处理方法封装在AMRMClientAsync.CallbackHandler中。 一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务；通常需要指定一个ContainerLaunchContext，提供Container启动时需要的信息。 NodeManager为任务设置好运行环境(包括环境变量、JAR包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务； 各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；ApplicationMaster与NodeManager的通信通过NMClientAsync object来完成，容器的所有事件通过NMClientAsync.CallbackHandler来处理。例如启动、状态更新、停止等。 应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。 YARN资源调度模型YARN提供了一个资源管理平台能够将集群中的资源统一进行管理。所有节点上的多维度资源都会根据申请抽象为一个个Container。 YARN采用了双层资源调度模型： ResourceManager中的资源调度器将资源分配给各个ApplicationMaster：资源分配过程是异步的。资源调度器将资源分配给一个应用程序后，它不会立刻push给对应的ApplicationMaster，而是暂时放到一个缓冲区中，等待ApplicationMaster通过周期性的心跳主动来取； ApplicationMaster领取到资源后再进一步分配给它内部的各个任务：不属于YARN平台的范畴，由用户自行实现。 也就是说，ResourceManager分配集群资源的时候，以抽象的Container形式分配给各应用程序，至于应用程序的子任务如何使用这些资源，由应用程序自行决定。 YARN目前采用的资源分配算法有三种。但真实的调度器实现中还对算法做了一定程度的优化。 Capacity Scheduler：该调度器用于在共享、多租户（multi-tenant）的集群环境中运行Hadoop应用，对运营尽可能友好的同时最大化吞吐量和效用。 该调度器保证共享集群的各个组织能够得到容量的保证，同时可以超额使用集群中暂时没有人使用的资源。Capacity Scheduler为了实现这些目标，抽象了queue的概念，queue通常由管理员配置。为了进一步细分容量的使用，调度器支持层级化的queue（hierarchical queues），使得在特定组织内部，可以进一步有效利用集群资源。 Capacity调度器支持的一些特性如下： 层级队列（Hierarchical Queues） 容量保证 安全性：每个队列都有队列的访问权限控制（ACL） 弹性： 空闲资源可以额外分配给任何需要的队列 多租户 基于资源的调度（resouce-based scheduling): 对资源敏感的应用程序，可以有效地控制资源情况 支持用户（组）到queue的映射：基于用户组提交作业到对应queue。 运营支持：支持运行时配置队列的容量，ACL等。也可以在运行时停止queue阻止进一步往queue提交作业。 Fair Scheduler：公平调度FAIR，该算法的思想是尽可能地公平调度，即已分配资源量少的优先级高。也就是说，在考虑如何分配资源时，调度器尽可能使得每个应用程序都能够得到大致相当的资源。默认情况下，公平性只通过内存来衡量，但是可以配置成内存和CPU。这种策略使得运行时间短的应用能够尽快结束，而不至于在等待资源时被饿死。另外，也可以为应用程序配置优先级，优先级用于决定资源使用量的占比。 参考文献 Hadoop 新MapReduce 框架Yarn 详解理解Hadoop YARN架构","raw":null,"content":null,"categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://comwork2016.github.io/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://comwork2016.github.io/tags/hadoop/"},{"name":"yarn","slug":"yarn","permalink":"https://comwork2016.github.io/tags/yarn/"}]},{"title":"Hadoop之MapReduce原理","slug":"Hadoop之MapReduce原理","date":"2017-04-17T02:11:31.000Z","updated":"2017-04-17T09:08:56.245Z","comments":true,"path":"2017/04/17/Hadoop之MapReduce原理/","link":"","permalink":"https://comwork2016.github.io/2017/04/17/Hadoop之MapReduce原理/","excerpt":"Hadoop Map/Reduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。","text":"Hadoop Map/Reduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。 概述一个Map/Reduce作业（job）通常会把输入的数据集切分为若干独立的数据块，由map任务（task）以完全并行的方式处理它们。框架会对map的输出先进行排序，然后把结果输入给reduce任务。通常作业的输入和输出都会被存储在文件系统中。整个框架负责任务的调度和监控，以及重新执行已经失败的任务。 通常，Map/Reduce框架和分布式文件系统是运行在一组相同的节点上的，也就是说，计算节点和存储节点通常在一起。这种配置允许框架在那些已经存好数据的节点上高效地调度任务，这可以使整个集群的网络带宽被非常高效地利用。 Map/Reduce框架由一个单独的master JobTracker和每个集群节点一个slave TaskTracker共同组成。master负责调度构成一个作业的所有任务，这些任务分布在不同的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由master指派的任务。 MapReduce的架构和HDFS一样，MapReduce也是采用Master/Slave的架构，其架构图如下所示。 MapReduce包含四个组成部分，分别为Client、JobTracker、TaskTracker和Task，下面我们详细介绍这四个组成部分。 Client 客户端每一个Job 都会在用户端通过Client类将应用程序以及配置参数Configuration打包成JAR文件存储在HDFS，并把路径提交到JobTracker的master服务，然后由master创建每一个Task（即MapTask 和ReduceTask）将它们分发到各个TaskTracker服务中去执行。 JobTrackerJobTracke负责资源监控和作业调度。JobTracker监控所有TaskTracker与job的健康状况，一旦发现失败，就将相应的任务转移到其他节点；同时，JobTracker会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在Hadoop 中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。 TaskTrackerTaskTracker会周期性地通过Heartbeat将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker 使用”slot”等量划分本节点上的资源量。”slot”代表计算资源（CPU、内存等）。一个Task获取到一个slot后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot分为Map slot和Reduce slot两种，分别供Map Task和Reduce Task使用。TaskTracker通过slot数目（可配置参数）限定Task的并发度。 TaskTask 分为Map Task和Reduce Task两种，均由TaskTracker启动。HDFS以固定大小的block为基本单位存储数据，而对于MapReduce而言，其处理单位是split。 Map Task执行过程如下图所示：由该图可知，Map Task先将对应的split迭代解析成一个个key/value 对，依次调用用户自定义的map()函数进行处理，最终将临时结果存放到本地磁盘上, 其中临时数据被分成若干个partition，每个partition将被一个Reduce Task处理。 Reduce Task执行过程下图所示。该过程分为三个阶段： 从远程节点上读取Map Task 中间结果（称为“Shuffle 阶段”）； 按照key 对key/value 对进行排序（称为“Sort 阶段”）； 依次读取&lt; key, value list&gt;，调用用户自定义的reduce() 函数处理，并将最终结果存到HDFS 上（称为“Reduce 阶段”）。 MapReduce运行机制下面从逻辑实体的角度介绍mapreduce运行机制，这些按照时间顺序包括：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。 在MapReduce运行过程中，最重要的是Map，Shuffle，Reduce三个阶段，各个阶段的作用简述如下： Map:数据输入,做初步的处理,输出形式的中间结果； Shuffle:按照partition、key对中间结果进行排序合并,输出给reduce线程； Reduce:对相同key的输入进行最终的处理,并将结果写入到文件中。 上图是把MapReduce过程分为两个部分，而实际上从两边的Map和Reduce到中间的那一大块都属于Shuffle过程，也就是说，Shuffle过程有一部分是在Map端，有一部分是在Reduce端。 输入分片（input split）InputFormatInputFormat为Map/Reduce作业描述输入的细节规范。Map/Reduce框架根据作业的InputFormat来进行以下操作： 检查作业输入的有效性。 把输入文件切分成多个逻辑InputSplit实例，并把每一实例分别分发给一个Mapper。 提供RecordReader的实现，这个RecordReader从逻辑InputSplit中获得输入记录，这些记录将由Mapper处理。 InputFormat只包含了两个接口函数: 123InputSplit[] getSplits(JobConf job, int numSplits) throws IOException;RecordReader &lt; K, V&gt; getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException; getSplits就是现在要使用的划分函数。job参数是任务的配置集合，从中可以取到用户在启动MapReduce时指定的输入文件路径。而numSplits参数是一个Split数目的建议值，是否考虑这个值，由具体的InputFormat实现来决定。返回的是InputSplit数组，它描述了所有的Split信息，一个InputSplit描述一个Split。 getRecordReader方法返回一个RecordReader对象，该对象将输入的InputSplit解析成若干个key/value对，MapReduce框架在Map Task执行过程中，会不断的调用RecordReader对象中的方法，迭代获取key/value对并交给map函数处理。 InputSplitInputSplit是一个单独的Mapper要处理的数据块。一般的InputSplit是字节样式输入，然后由RecordReader处理并转化成记录样式。InputSplit也只有两个接口函数： 123long getLength() throws IOException;String[] getLocations() throws IOException; 这个interface仅仅描述了Split有多长，以及存放这个Split的Location信息（也就是这个Split在HDFS上存放的机器。它可能有多个replication，存在于多台机器上）。除此之外，就再没有任何直接描述Split的信息了。 而Split中真正重要的描述信息还是只有InputFormat会关心。在需要读取一个Split的时候，其对应的InputSplit会被传递到InputFormat的第二个接口函数getRecordReader，然后被用于初始化一个RecordReader，以解析输入数据。 在分配Map任务时，Split的Location信息就要发挥作用了。JobTracker会根据TaskTracker的地址来选择一个Location与之最接近的Split所对应的Map任务（注意一个Split可以有多个Location）。这样一来，输入文件中Block的Location信息经过一系列的整合（by InputFormat）和传递，最终就影响到了Map任务的分配。其结果是Map任务倾向于处理存放在本地的数据，以保证效率。 RecordReaderRecordReader从InputSlit读入&lt;key, value&gt;对。 一般的，RecordReader 把由InputSplit 提供的字节样式的输入文件，转化成由Mapper处理的记录样式的文件。因此RecordReader负责处理记录的边界情况和把数据表示成&lt;keys, values&gt;对形式。 Map阶段在进行海量数据处理时，外存文件数据I/O访问会成为一个制约系统性能的瓶颈，因此，Hadoop的Map过程实现的一个重要原则就是：计算靠近数据，这里主要指两个方面： 代码靠近数据： 原则：本地化数据处理（locality），即一个计算节点尽可能处理本地磁盘上所存储的数据； 尽量选择数据所在DataNode启动Map任务； 这样可以减少数据通信，提高计算效率； 数据靠近代码： 当本地没有数据处理时，尽可能从同一机架或最近其他节点传输数据进行处理（host选择算法）。 map的经典流程图如下： 输入 map task只读取split分片，split与block（HDFS的最小存储单位，默认为64MB）可能是一对一也能是一对多，但是对于一个split只会对应一个文件的一个block或多个block，不允许一个split对应多个文件的多个block； 这里切分和输入数据的时会涉及到InputFormat的文件切分算法和host选择算法。 文件切分算法，主要用于确定InputSplit的个数以及每个InputSplit对应的数据段。FileInputFormat以文件为单位切分生成InputSplit，对于每个文件，由以下三个属性值决定其对应的InputSplit的个数： goalSize： 它是根据用户期望的InputSplit数目计算出来的，即totalSize/numSplits。其中，totalSize为文件的总大小；numSplits为用户设定的Map Task个数，默认情况下是1； minSize：InputSplit的最小值，由配置参数mapred.min.split.size确定，默认是1； blockSize：文件在hdfs中存储的block大小，不同文件可能不同，默认是64MB。 这三个参数共同决定InputSplit的最终大小，计算方法如下：$$splitSize=\\max(minSize, \\min(gogalSize,blockSize))$$ Partitioner作用：将map的结果发送到相应的reduce端，总的partition的数目等于reducer的数量。 实现功能： map输出的是&lt;key,value&gt;对，决定于当前的mapper的partition交给哪个reduce的方法是：mapreduce提供的Partitioner接口，对key进行hash后，再以reducetask数量取模，然后到指定的job上（HashPartitioner，可以通过job.setPartitionerClass(MyPartition.class)自定义）。 然后将数据写入到内存缓冲区，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。&lt;key,value&gt;对以及Partition的结果都会被写入缓冲区。在写入之前，key与value值都会被序列化成字节数组。 要求：负载均衡，效率； spill（溢写）作用：把内存缓冲区中的数据写入到本地磁盘，在写入本地磁盘时先按照partition、再按照key进行排序（quick sort）； 注意： 这个spill是由另外单独的线程来完成，不影响往缓冲区写map结果的线程； 内存缓冲区默认大小限制为100MB，它有个溢写比例（spill.percent），默认为0.8，当缓冲区的数据达到阈值时，溢写线程就会启动，先锁定这80MB的内存，执行溢写过程，maptask的输出结果还可以往剩下的20MB内存中写，互不影响。然后再重新利用这块缓冲区，因此Map的内存缓冲区又叫做环形缓冲区（两个指针的方向不会变，下面会详述）； 在将数据写入磁盘之前，先要对要写入磁盘的数据进行一次排序操作，先按&lt;key,value,partition&gt;中的partition分区号排序，然后再按key排序，这个就是sort操作，最后溢出的小文件是分区的，且同一个分区内是保证key有序的； Combinercombine：执行combine操作要求开发者必须在程序中设置了combine（程序中通过job.setCombinerClass(myCombine.class)自定义combine操作）。 程序中有两个阶段可能会执行combine操作： map输出数据根据分区排序完成后，在写入文件之前会执行一次combine操作（前提是作业中设置了这个操作）； 如果map输出比较大，溢出文件个数大于3（此值可以通过属性min.num.spills.for.combine配置）时，在merge的过程（多个spill文件合并为一个大文件）中还会执行combine操作； combine主要是把形如&lt;aa,1&gt;,&lt;aa,2&gt;这样的key值相同的数据进行计算，计算规则与reduce一致，比如：当前计算是求key对应的值求和，则combine操作后得到&lt;aa,3&gt;这样的结果。 注意事项：不是每种作业都可以做combine操作的，只有满足以下条件才可以： reduce的输入输出类型都一样，因为combine本质上就是reduce操作； 计算逻辑上，combine操作后不会影响计算结果，像求和就不会影响； merge当map很大时，每次溢写会产生一个spill_file，这样会有多个spill_file，而最终的一个map task输出只有一个文件，因此，最终的结果输出之前会对多个中间过程进行多次溢写文件（spill_file）的合并，此过程就是merge过程。也即是，待Map Task任务的所有数据都处理完后，会对任务产生的所有中间数据文件做一次合并操作，以确保一个Map Task最终只生成一个中间数据文件。 注意： 如果生成的文件太多，可能会执行多次合并，每次最多能合并的文件数默认为10，可以通过属性min.num.spills.for.combine配置； 多个溢出文件合并时，会进行一次排序，排序算法是多路归并排序； 是否还需要做combine操作，一是看是否设置了combine，二是看溢出的文件数是否大于等于3； 最终生成的文件格式与单个溢出文件一致，也是按分区顺序存储，并且输出文件会有一个对应的索引文件，记录每个分区数据的起始位置，长度以及压缩长度，这个索引文件名叫做file.out.index。 内存缓冲区在Map Task任务的业务处理方法map()中，最后一步通过OutputCollector.collect(key,value)或context.write(key,value)输出Map Task的中间处理结果，在相关的collect(key,value)方法中，会调用Partitioner.getPartition(K2 key, V2 value, int numPartitions)方法获得输出的&lt;key,value&gt;对应的分区号(分区号可以认为对应着一个要执行Reduce Task的节点)，然后将&lt;key,value,partition&gt;暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，该缓冲区的默认大小是100MB，可以通过参数io.sort.mb来调整其大小。 当缓冲区中的数据使用率达到一定阀值后，触发一次Spill操作，将环形缓冲区中的部分数据写到磁盘上，生成一个临时的Linux本地数据的spill文件；然后在缓冲区的使用率再次达到阀值后，再次生成一个spill文件。直到数据处理完毕，在磁盘上会生成很多的临时文件。 缓存有一个阀值比例配置，当达到整个缓存的这个比例时，会触发spill操作；触发时，map输出还会接着往剩下的空间写入，但是写满的空间会被锁定，数据溢出写入磁盘。当这部分溢出的数据写完后，空出的内存空间可以接着被使用，形成像环一样的被循环使用的效果，所以又叫做环形内存缓冲区； Reduce阶段Reduce过程的经典流程图如下： copy作用：拉取数据； 过程：Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为这时map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。 默认情况下，当整个MapReduce作业的所有已执行完成的Map Task任务数超过Map Task总数的5%后，JobTracker便会开始调度执行Reduce Task任务。然后Reduce Task任务默认启动mapred.reduce.parallel.copies(默认为5）个MapOutputCopier线程到已完成的Map Task任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，则写到磁盘上。 mergeCopy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。 这里需要强调的是，merge有三种形式：1)内存到内存 2)内存到磁盘 3)磁盘到磁盘。默认情况下第一种形式是不启用的。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge（图中的第一个merge，之所以进行merge是因为reduce端在从多个map端copy数据的时候，并没有进行sort，只是把它们加载到内存，当达到阈值写入磁盘时，需要进行merge） 。这和map端的很类似，这实际上就是溢写的过程，在这个过程中如果你设置有Combiner，它也是会启用的，然后在磁盘中生成了众多的溢写文件，这种merge方式一直在运行，直到没有 map 端的数据时才结束，然后才会启动第三种磁盘到磁盘的 merge （图中的第二个merge）方式生成最终的那个文件。 在远程copy数据的同时，Reduce Task在后台启动了两个后台线程对内存和磁盘上的数据文件做合并操作，以防止内存使用过多或磁盘生的文件过多。 Reducemerge的最后会生成一个文件，大多数情况下存在于磁盘中，但是需要将其放入内存中。当reducer 输入文件已定，整个 Shuffle 阶段才算结束。然后就是 Reducer 执行，把结果放到 HDFS 上。 Reduce的数目建议是0.95或1.75乘以 (&lt;no. of nodes&gt; * mapred.tasktracker.reduce.tasks.maximum)。 用0.95，所有reduce可以在maps一完成时就立刻启动，开始传输map的输出结果。用1.75，速度快的节点可以在完成第一轮reduce任务后，可以开始第二轮，这样可以得到比较好的负载均衡的效果。 如果没有归约要进行，那么设置reduce任务的数目为零是合法的。这种情况下，map任务的输出会直接被写入由 setOutputPath(Path)指定的输出路径。框架在把它们写入FileSystem之前没有对它们进行排序。 参考文献 Hadoop Map/Reduce教程深入理解MapReduce的架构及原理Hadoop InputFormat浅析–hadoop如何分配输入MapReduce之Shuffle过程详述","raw":null,"content":null,"categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://comwork2016.github.io/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://comwork2016.github.io/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"https://comwork2016.github.io/tags/mapreduce/"}]},{"title":"Hadoop之HDFS架构","slug":"Hadoop之HDFS架构","date":"2017-04-14T07:27:16.000Z","updated":"2017-04-16T12:38:04.453Z","comments":true,"path":"2017/04/14/Hadoop之HDFS架构/","link":"","permalink":"https://comwork2016.github.io/2017/04/14/Hadoop之HDFS架构/","excerpt":"HDFS即Hadoop Distributed File System分布式文件系统，它的设计目标是把超大数据集存储到分布在网络中的多台普通商用计算机上，并且能够提供高可靠性和高吞吐量的服务。","text":"HDFS即Hadoop Distributed File System分布式文件系统，它的设计目标是把超大数据集存储到分布在网络中的多台普通商用计算机上，并且能够提供高可靠性和高吞吐量的服务。 HDFS架构原理HDFS主要由3个组件构成，分别是NameNode、SecondaryNameNode和DataNode，HDFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode 运行在master节点，DataNode运行slave节点。 数据块磁盘数据块是磁盘读写的基本单位，与普通文件系统类似，hdfs也会把文件分块来存储。hdfs默认数据块大小为64MB，磁盘块一般为512B，HDFS块为何如此之大呢？块增大可以减少寻址时间与文件传输时间的比例，若寻址时间为10ms，磁盘传输速率为100MB/s，那么寻址与传输比仅为1%。当然，磁盘块太大也不好，因为一个MapReduce通常以一个块作为输入，块过大会导致整体任务数量过小，降低作业处理速度。 hdfs按块存储还有如下好处： 文件可以任意大，也不用担心单个结点磁盘容量小于文件的情况 简化了文件子系统的设计，子系统只存储文件块数据，而文件元数据则交由其它系统（NameNode）管理 有利于备份和提高系统可用性，因为可以以块为单位进行备份，hdfs默认备份数量为3。 有利于负载均衡 NameNodeNameNode中的元信息当一个客户端请求一个文件或者存储一个文件时，它需要先知道具体到哪个DataNode上存取，获得这些信息后，客户端再直接和这个DataNode进行交互，而这些信息的维护者就是NameNode。 NameNode管理着文件系统命名空间，它维护这文件系统树及树中的所有文件和目录。NameNode也负责维护所有这些文件或目录的打开、关闭、移动、重命名等操作。对于实际文件数据的保存与操作，都是由DataNode负责。当一个客户端请求数据时，它仅仅是从NameNode中获取文件的元信息，而具体的数据传输不需要经过NameNode，是由客户端直接与相应的DataNode进行交互。 NameNode保存元信息的种类有： 文件名目录名及它们之间的层级关系 文件目录的所有者及其权限 每个文件块的名及文件有哪些块组成 需要注意的是，NameNode元信息并不包含每个块的位置信息，这些信息会在NameNode启动时从各个DataNode获取并保存在内存中，因为这些信息会在系统启动时由数据节点重建。把块位置信息放在内存中，在读取数据时会减少查询时间，增加读取效率。 元信息的持久化在NameNode中存放元信息的文件是fsimage。在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个文件edits中。并且edits文件和fsimage文件会被SecondaryNameNode周期性的合并（合并过程会在SecondaryNameNode中详细介绍）。 其它问题运行NameNode会占用大量内存和I/O资源，一般NameNode不会存储用户数据或执行MapReduce任务。 为了简化系统的设计，Hadoop只有一个NameNode，这也就导致了hadoop集群的单点故障问题。因此，对NameNode节点的容错尤其重要，hadoop提供了如下两种机制来解决： 将hadoop元数据写入到本地文件系统的同时再实时同步到一个远程挂载的网络文件系统（NFS）。 运行一个secondaryNameNode，它的作用是与NameNode进行交互，定期通过编辑日志文件合并命名空间镜像，当NameNode发生故障时它会通过自己合并的命名空间镜像副本来恢复。需要注意的是secondaryNameNode保存的状态总是滞后于NameNode，所以这种方式难免会导致丢失部分数据（后面会详细介绍）。 SecondaryNameNode需要注意，SecondaryNameNode并不是NameNode的备份。我们从前面的介绍已经知道，所有HDFS文件的元信息都保存在NameNode的内存中。在NameNode启动时，它首先会加载fsimage到内存中，在系统运行期间，所有对NameNode的操作也都保存在了内存中，同时为了防止数据丢失，这些操作又会不断被持久化到本地edits文件中。 Edits文件存在的目的是为了提高系统的操作效率，NameNode在更新内存中的元信息之前都会先将操作写入edits文件。在NameNode重启的过程中，edits会和fsimage合并到一起，但是合并的过程会影响到Hadoop重启的速度，SecondaryNameNode就是为了解决这个问题而诞生的。 SecondaryNameNode的角色就是定期的合并edits和fsimage文件，我们来看一下合并的步骤： 合并之前告知NameNode把所有的操作写到新的edites文件并将其命名为edits.new。 SecondaryNameNode从NameNode请求fsimage和edits文件 SecondaryNameNode把fsimage和edits文件合并成新的fsimage文件 NameNode从SecondaryNameNode获取合并好的新的fsimage并将旧的替换掉，并把edits用第一步创建的edits.new文件替换掉 更新fstime文件中的检查点 最后再总结一下整个过程中涉及到NameNode中的相关文件 fsimage ：保存的是上个检查点的HDFS的元信息 edits ：保存的是从上个检查点开始发生的HDFS元信息状态改变信息 fstime：保存了最后一个检查点的时间戳 DataNodeDataNode是HDFS中的worker节点，它负责存储数据块，也负责为系统客户端提供数据块的读写服务，同时还会根据NameNode的指示来进行创建、删除、和复制等操作。此外，它还会通过心跳定期向NameNode发送所存储文件块列表信息。当对hdfs文件系统进行读写时，NameNode告知客户端每个数据驻留在哪个DataNode，客户端直接与DataNode进行通信，DataNode还会与其它DataNode通信，复制这些块以实现冗余。 数据备份HDFS通过备份数据块的形式来实现容错，除了文件的最后一个数据块外，其它所有数据块大小都是一样的。NameNode负责各个数据块的备份，DataNode会通过心跳的方式定期的向NameNode发送自己节点上的Block报告，这个报告中包含了DataNode节点上的所有数据块的列表。 一个大型的HDFS文件系统一般都是需要跨很多机架的，不同机架之间的数据传输需要经过网关，并且，同一个机架中机器之间的带宽要大于不同机架机器之间的带宽。如果把所有的副本都放在不同的机架中，这样既可以防止机架失败导致数据块不可用，又可以在读数据时利用到多个机架的带宽，并且也可以很容易的实现负载均衡。但是，如果是写数据，各个数据块需要同步到不同的机架，会影响到写数据的效率。而在Hadoop中，如果副本数量是3的情况下，Hadoop默认是这么存放的，把第一个副本放到机架的一个节点上，另一个副本放到同一个机架的另一个节点上，把最后一个节点放到不同的机架上。这种策略减少了跨机架副本的个数提高了写的性能，也能够允许一个机架失败的情况，算是一个很好的权衡。 安全模式关于安全模式，当 Hadoop的NameNode节点启动时，会进入安全模式阶段。在此阶段，DataNode会向NameNode上传它们数据块的列表，让 NameNode得到块的位置信息，并对每个文件对应的数据块副本进行统计。当最小副本条件满足时，即一定比例的数据块都达到最小副本数，系统就会退出安全模式，而这需要一定的延迟时间。当最小副本条件未达到要求时，就会对副本数不足的数据块安排DataNode进行复制，直至达到最小副本数。而在安全模式下，系统会处于只读状态，NameNode不会处理任何块的复制和删除命令。 HDFS负载均衡HDFS的数据也许并不是非常均匀的分布在各个DataNode中。一个常见的原因是在现有的集群上经常会增添新的DataNode节点。当新增一个数据块（一个文件的数据被保存在一系列的块中）时，NameNode在选择DataNode接收这个数据块之前，会考虑到很多因素。其中的一些考虑的是： 将数据块的一个副本放在正在写这个数据块的节点上。 尽量将数据块的不同副本分布在不同的机架上，这样集群可在完全失去某一机架的情况下还能存活。 一个副本通常被放置在和写文件的节点同一机架的某个节点上，这样可以减少跨越机架的网络I/O。 尽量均匀地将HDFS数据分布在集群的DataNode中。 HDFS健壮性HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是： Namenode出错 , Datanode出错和网络割裂(network partitions)。 磁盘数据错误，心跳检测和重新复制每个Datanode节点周期性地向Namenode发送心跳信号。网络割裂可能导致一部分Datanode跟 Namenode失去联系。Namenode通过心跳信号的缺失来检测这一情况，并将这些近期不再发送心跳信号 Datanode标记为宕机，不会再将新的IO请求发给它们。任何存储在宕机Datanode上的数据将不再有效。Datanode的宕机可能会引起一些数据块的副本系数低于指定值，Namenode不断地检测这些需要复制的数据块，一旦发现就启动复制操作。 数据完整性从某个Datanode获取的数据块有可能是损坏的，损坏可能是由Datanode的存储设备错误、网络错误或者软件bug造成的。HDFS客户端软件实现了对HDFS文件内容的校验和(checksum)检查。当客户端创建一个新的HDFS文件，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个HDFS名字空间下。当客户端获取文件内容后，它会检验从Datanode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他Datanode 获取该数据块的副本。 元数据磁盘错误FsImage和Editlog是HDFS的核心数据结构。如果这些文件损坏了，整个HDFS 实例都将失效。因而，Namenode可以配置成支持维护多个FsImage和Editlog的副本。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这种多副本的同步操作可能会降低Namenode每秒处理的名字空间事 务数量。然而这个代价是可以接受的，因为即使HDFS的应用是数据密集的，它们也非元数据密集的。当 Namenode重启的时候，它会选取最近的完整的FsImage和Editlog来使用。 Namenode是HDFS集群中的单点故障(single point of failure)所在。如果Namenode机器故障，是需要手工干预的。目前，自动重启或在另一台机器上做Namenode故障转移的功能还没实现。 HDFS网络HDFS中的沟通协议所有的HDFS中的沟通协议都是基于tcp/ip协议，一个客户端通过指定的tcp端口与NameNode机器建立连接，并通过ClientProtocol协议与NameNode交互。而DataNode则通过DataNode Protocol协议与NameNode进行沟通。HDFS的RCP(远程过程调用)对ClientProtocol和DataNode Protocol做了封装。按照HDFS的设计，NameNode不会主动发起任何请求，只会被动接受来自客户端或DataNode的请求。 HDFS机架感知与网络拓扑通常，大型Hadoop集群是以机架的形式来组织的，而HDFS不能够自动判断集群中各个datanode的网络拓扑情况，因此Hadoop允许集群的管理员通过配置dfs.network.script参数来确定节点所处的机架。 文件提供了IP-&gt;rackid的翻译。NameNode通过这个得到集群中各个datanode机器的rackid。如果topology.script.file.name没有设定，则每个IP都会翻译 成/default-rack。 Hadoop把网络拓扑看成是一棵树，两个节点的距离=它们到最近共同祖先距离的总和，而树的层次可以这么划分： 同一节点中的进程 同一机架上的不同节点 同一数据中心不同机架 不同数据中心的节点 若数据中心$d_1$中一个机架$r_1$中一个节点$n_1$表示为$d_1/r_1/n_1$,则： $$ distance(d_1/r_1/n_1,d_1/r_1/n_1)=0; 相同的datanode\\\\ distance(d_1/r_1/n_1,d_1/r_1/n_2)=2; 同一rack下的不同datanode\\\\ distance(d_1/r_1/n_1,d_1/r_2/n_3)=4; 同一IDC下的不同datanode\\\\ distance(d_1/r_1/n_1,d_2/r_3/n_4)=6; 不同IDC下的datanode$$ hdfs文件读写过程剖析 hdfs文件读取过程HDFS有一个FileSystem实例，客户端通过调用这个实例的open()方法就可以打开系统中希望读取的文件。HDFS通过rpc调用NameNode获取文件块的位置信息，对于文件的每一个块，NameNode会返回含有该块副本的DataNode的节点地址，另外，客户端还会根据网络拓扑来确定它与每一个DataNode的位置信息，从离它最近的那个DataNode获取数据块的副本，最理想的情况是数据块就存储在客户端所在的节点上。 HDFS会返回一个FSDataInputStream对象，FSDataInputStream类转而封装成DFSDataInputStream对象,这个对象管理着与DataNode和NameNode的I/O，具体过程是： 客户端发起读请求 客户端与NameNode得到文件的块及位置信息列表 客户端直接和DataNode交互读取数据 读取完成关闭连接 当FSDataInputStream与DataNode通信时遇到错误，它会选取另一个较近的DataNode，并为出故障的DataNode做标记以免重复向其读取数据。FSDataInputStream还会对读取的数据块进行校验和确认，发现块损坏时也会重新读取并通知NameNode。 这样设计的巧妙之处： 让客户端直接联系DataNode检索数据，可以使HDFS扩展到大量的并发客户端，因为数据流就是分散在集群的每个节点上的，在运行MapReduce任务时，每个客户端就是一个DataNode节点。 NameNode仅需相应块的位置信息请求（位置信息在内存中，速度极快），否则随着客户端的增加，NameNode会很快成为瓶颈。 HDFS文件写入过程HDFS有一个DistributedFileSystem实例，客户端通过调用这个实例的create()方法就可以创建文件。DistributedFileSystem会发送给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件，在创建文件前NameNode会做一些检查，如文件是否存在，客户端是否有创建权限等，若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog，若不通过会向客户端抛出IOException。创建成功之后DistributedFileSystem会返回一个FSDataOutputStream对象，客户端由此开始写入数据。 同读文件过程一样，FSDataOutputStream类转而封装成DFSDataOutputStream对象,这个对象管理着与DataNode和NameNode的I/O，具体过程是： 客户端在向NameNode请求之前先写入文件数据到本地文件系统的一个临时文件 待临时文件达到块大小时开始向NameNode请求DataNode信息 NameNode在文件系统中创建文件并返回给客户端一个数据块及其对应DataNode的地址列表（列表中包含副本存放的地址） 客户端通过上一步得到的信息把创建临时文件块flush到列表中的第一个DataNode 当文件关闭，NameNode会提交这次文件创建，此时，文件在文件系统中可见 上面第四步描述的flush过程实际处理过程比较负杂，现在单独描述一下： 首先，第一个DataNode是以数据包(数据包一般4KB)的形式从客户端接收数据的，DataNode在把数据包写入到本地磁盘的同时会向第二个DataNode（作为副本节点）传送数据。 在第二个DataNode把接收到的数据包写入本地磁盘时会向第三个DataNode发送数据包 第三个DataNode开始向本地磁盘写入数据包。此时，数据包以流水线的形式被写入和备份到所有DataNode节点 传送管道中的每个DataNode节点在收到数据后都会向前面那个DataNode发送一个ACK,最终，第一个DataNode会向客户端发回一个ACK 当客户端收到数据块的确认之后，数据块被认为已经持久化到所有节点。然后，客户端会向NameNode发送一个确认 如果管道中的任何一个DataNode失败，管道会被关闭。数据将会继续写到剩余的DataNode中。同时NameNode会被告知待备份状态，NameNode会继续备份数据到新的可用的节点 数据块都会通过计算校验和来检测数据的完整性，校验和以隐藏文件的形式被单独存放在hdfs中，供读取时进行完整性校验 hdfs文件删除过程hdfs文件删除过程一般需要如下几步： 一开始删除文件，NameNode只是重命名被删除的文件到/trash目录，因为重命名操作只是元信息的变动，所以整个过程非常快。在/trash中文件会被保留一定间隔的时间（可配置，默认是6小时），在这期间，文件可以很容易的恢复，恢复只需要将文件从/trash移出即可。 当指定的时间到达，NameNode将会把文件从命名空间中删除 标记删除的文件块释放空间，HDFS文件系统显示空间增加 HDFS缺点一般来说，一条元信息记录会占用200byte内存空间。假设块大小为64MB，备份数量是3 ，那么一个1GB大小的文件将占用1GB/64MB*3=48个文件块。如果现在有1000个1MB大小的文件，则会占用1000*3=3000个文件块（多个文件不能放到一个块中）。我们可以发现，如果文件越小，存储同等大小文件所需要的元信息就越多，所以，Hadoop更喜欢大文件。 还有一个问题就是，因为 Map task 的数量是由 splits 来决定的，所以 用 MR 处理大量的小文件时，就会产生过多的 Maptask ，线程管理开销将会增加作业时间。举个例子，处理10000M的文件，若每个split为1M ，那就会有10000个Maptasks，会有很大的线程开销；若每个split为 100M，则只有100个Maptasks，每个Maptask 将会有更多的事情做，而线程的管理开销也将减小很多。 参考文献 HDFS 原理、架构与特性介绍Hadoop核心之HDFS 架构设计","raw":null,"content":null,"categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://comwork2016.github.io/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://comwork2016.github.io/tags/hadoop/"},{"name":"hdfs","slug":"hdfs","permalink":"https://comwork2016.github.io/tags/hdfs/"}]},{"title":"Hadoop 2.7.3 安装","slug":"Hadoop-2-7-3-安装","date":"2017-04-14T01:10:37.000Z","updated":"2017-04-14T07:11:40.099Z","comments":true,"path":"2017/04/14/Hadoop-2-7-3-安装/","link":"","permalink":"https://comwork2016.github.io/2017/04/14/Hadoop-2-7-3-安装/","excerpt":"在此记录hadoop 2.7.3版本的安装过程以及基本配置过程。","text":"在此记录hadoop 2.7.3版本的安装过程以及基本配置过程。 安装环境 jdk1.8 hadoop 2.7.3 CentOS release 6.7 (Final) * 3 hostname ip master 172.168.170.84 slave1 172.168.170.88 slave2 172.168.170.89 必需软件 JDK安装(下载地址) ssh安装 hadoop中使用ssh来实现cluster中各个node的登陆认证，同时需要进行ssh免密登陆。 1sudo apt-get install ssh rsync安装 Ubuntu 12.10已经自带rsync。 1sudo apt-get install rsync hadoop下载 从官方mirrors下载对应版本的hadoop。 安装Hadoop 创建hadoop用户组以及用户 12sudo addgroup hadoopsudo adduser --ingroup hadoop hadoop 重新用hadoop用户登陆到Linux中。 将hadoop解压到目录/home/hadoop/local/opt中 配置hadoop环境变量 12345678910export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk.x86_64/export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport HADOOP_HOME=$HOME/local/opt/hadoop-2.7.3export HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOMEexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 进入hadoop-2.7.3/etc/hadoop文件夹修改core-site.xml文件 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/local/var/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml文件 123456789101112131415161718192021222324&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:9001&lt;/value&gt; &lt;description&gt;# 通过web界面来查看HDFS状态 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/local/var/hadoop/hdfs/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/local/var/hadoop/hdfs/datanode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;description&gt;# 每个Block有1个备份&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-site.xml 这个是mapreduce任务的配置，由于hadoop2.x使用了yarn框架，所以要实现分布式部署，必须在mapreduce.framework.name属性下配置为yarn。mapred.map.tasks和mapred.reduce.tasks分别为map和reduce的任务数。 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改yarn-site.xml 1234567891011121314151617181920212223242526272829303132333435&lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改slaves文件 12slave1slave2 修改hosts文件，命名各个节点的名称。 1234127.0.0.1 localhost172.168.170.84 master172.168.170.88 slave1172.168.170.89 slave2 节点之间ssh免密登陆在master节点中生成密钥，并添加到.ssh/authorized_keys文件中。 12ssh-keygen -t rsacat id_rsa.pub&gt;&gt; authorized_keys 将master中的/etc/hosts文件和.ssh/authorized_keys文件发送到slave1和slave2文件中。 123scp /etc/hosts hadoop@slave1:/etc/hostsscp /home/hadoop/.ssh/authorized_keys hadoop@slave1:/home/hadoop/.ssh/authorized_keysscp /home/hadoop/.ssh/authorized_keys hadoop@slave2:/home/hadoop/.ssh/authorized_keys 完成之后可以利用以下语句测试免密登陆。 12ssh slave1ssh slave2 将hadoop-2.7.3文件拷贝至slave1和slave2 12scp -r /home/hadoop/local/opt/hadoop-2.7.3 hadoop@slave1:/home/hadoop/local/opt/scp -r /home/hadoop/local/opt/hadoop-2.7.3 hadoop@slave2:/home/hadoop/local/opt/ 启动Hadoop 在master节点使用hadoop用户初始化NameNode 123hdfs namenode –format#执行后控制台输出，看到 Exiting with status 0 表示格式化成功。#如有错误，先删除var目录下的临时文件，然后重新运行该命令 启动hadoop 1234#启动hdfsstart-dfs.sh#启动yarn分布式计算框架start-yarn.sh 用jps命令查看hadoop集群运行情况 master节点 12345JpsNameNodeResourceManagerSecondaryNameNodeJobHistoryServer slave节点 123JpsDataNodeNodeManager 通过以下网址查看集群状态 12http://172.168.170.84:50070http://172.168.170.84:8088","raw":null,"content":null,"categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://comwork2016.github.io/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://comwork2016.github.io/tags/hadoop/"}]},{"title":"隐性语义索引（Latent Semantic Indexing）","slug":"隐性语义索引（Latent-Semantic-Indexing）","date":"2017-04-12T02:33:12.000Z","updated":"2017-04-13T11:34:28.082Z","comments":true,"path":"2017/04/12/隐性语义索引（Latent-Semantic-Indexing）/","link":"","permalink":"https://comwork2016.github.io/2017/04/12/隐性语义索引（Latent-Semantic-Indexing）/","excerpt":"隐性语义索引(LSI)采用线性代数中的奇异值分解方法，选取前几个比较大的奇异值所对应的特征向量将原矩阵映射到低维空间中，从而达到词矢量的目的。","text":"隐性语义索引(LSI)采用线性代数中的奇异值分解方法，选取前几个比较大的奇异值所对应的特征向量将原矩阵映射到低维空间中，从而达到词矢量的目的。 奇异矩阵分解SVD奇异矩阵分解SVD的内容见上文博客。 词项—文档矩阵及SVD词项-文档矩阵是一个由$M$个词项和$N$篇文档组成的$M×N$的权重矩阵$C$，矩阵的每行代表一个词项，每列代表一篇文档。然而，我们感兴趣的是$M×N$的词项—文档矩阵$C$，一般有$M\\neq N$。这个矩阵经过SVD分解之后的形势如下式所示： $$\\begin{equation}CC^T =UΣV^TVΣ^TU^T =UΣΣ^TU^T\\end{equation}$$ 那么左边的$CC^T$ 代表什么呢？它实际上是一个方阵，其每行和每列都对应$M$个词项中的一个。矩阵中的第$i$行、第$j$列的元素实际上是第$i$个词项与第$j$个词项基于文档共现次数的一个重合度计算指标（可以从矩阵的乘法中推断出来）。其精确的数学含义依赖于构建$C$所使用的词项权重方法。假定$C$是词项-文档出现矩阵，那么$CC^T$的第$i$行、第$j$列的元素是词项$i$和词项$j$共现的文档数目。 低秩逼近小特征值对于矩阵乘法的影响也小。因此，将这些小特征值替换成0将不会对最后的乘积有实质性影响，也就是说该乘积接近$C$。SVD可以用于解决矩阵低秩逼近问题，主要操作分为以下三步： 给定$C$，构造SVD分解有$C = UΣV^T$； 把$Σ$中对角线上$r-k$个最小奇异值置为0，从而得到$Σ_k$； 计算$C_k = UΣ_kV^T$作为$C$的逼近。 潜在语义索引LSI空间向量模型可以将查询和文档转换成同一空间下的向量，可以基于余弦相似度进行评分计算，能够对不同的词项赋予不同的权重，除了文档检索之外还可以推广到诸如聚类和分类等其他领域，等等。但是，向量空间表示方法没有能力处理自然语言中的两个经典问题：一义多词（synonymy）和一词多义（polysemy）问题。一个很自然的问题就是，能否利用词项的共现情况（比如，charge是和steed 还是electron在某篇文档中共现），来获得词项的隐性语义关联从而减轻这些问题的影响？ 即使对一个中等规模的文档集来说，词项-文档矩阵$C$也可能有成千上万个行和列，它的秩数目大概也是这个数量级。在LSI中，我们使用SVD分解来构造$C$的一个低秩逼近$C_k$，其中$k$远小于矩阵$C$原始的秩。实践时$k$的取值往往在几百以内。这样，我们就可以将词项—文档矩阵中每行和每列（分别对应每个词项和每篇文档）映射到一个$k$维空间，为$CC^T$和$C^TC$的$k$个主特征向量（对应$k$个最大的特征值）可以定义该空间。需要注意的是，不管$k$取值如何，矩阵$C_k$仍然是一个$M × N$的矩阵。矩阵$U$被称为SVD词项矩阵（SVD term matrix）,$V^T$被称为SVD文档矩阵（SVD document matrix）。 举例考虑如下词项—文档矩阵 C = $d_1$ $d_2$ $d_3$ $d_4$ $d_5$ $d_6$ ship 1 0 1 0 0 0 boat 0 1 0 0 0 0 ocean 1 1 0 0 0 0 voyage 1 0 0 1 1 0 trip 0 0 0 1 0 1 利用SVD分解，可以将其分解生成三个矩阵的乘积。 计算$CC^T$的值为 $$\\begin{eqnarray}CC^T=\\begin{pmatrix}1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1\\end{pmatrix}\\begin{pmatrix}1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\end{pmatrix}=\\begin{pmatrix}2 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\1 &amp; 1 &amp; 2 &amp; 1 &amp; 0 \\\\1 &amp; 0 &amp; 1 &amp; 3 &amp; 1 \\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 2\\end{pmatrix}\\end{eqnarray}$$ $CC^T$对应的特征值和特征向量为： $$\\begin{equation}\\lambda_1=4.68,p_1=\\begin{pmatrix} 0.44 &amp; 0.13 &amp; 0.48 &amp; 0.70 &amp; 0.26 \\end{pmatrix}^T\\\\\\lambda_2=2.54,p_2=\\begin{pmatrix} 0.30 &amp; 0.33 &amp; 0.51 &amp; -0.35 &amp; -0.65 \\end{pmatrix}^T\\\\\\lambda_3=1.63,p_3=\\begin{pmatrix} 0.57 &amp; -0.59 &amp; -0.37 &amp; 0.15 &amp; -0.41 \\end{pmatrix}^T\\\\\\lambda_4=1,p_4=\\begin{pmatrix} 0.58 &amp; -0.00 &amp; -0.00 &amp; -0.58 &amp; 0.58 \\end{pmatrix}^T\\\\\\lambda_5=0.16,p_5=\\begin{pmatrix} -0.25 &amp; -0.73 &amp; 0.61 &amp; -0.16 &amp; 0.09 \\end{pmatrix}^T\\end{equation}$$ 计算$C^TC$的值为$$\\begin{eqnarray}C^TC=\\begin{pmatrix}1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\end{pmatrix}\\begin{pmatrix}1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1\\end{pmatrix}=\\begin{pmatrix}3 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\1 &amp; 2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\1 &amp; 0 &amp; 0 &amp; 2 &amp; 1 &amp; 1 \\\\1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1\\end{pmatrix}\\end{eqnarray}$$ $C^TC$对应的特征值和特征向量为： $$\\begin{equation}\\lambda_1=4.68,p_1=\\begin{pmatrix} -0.75 &amp; -0.28 &amp; -0.20 &amp; -0.45 &amp; -0.33 &amp; -0.12 \\end{pmatrix}^T\\\\\\lambda_2=2.54,p_2=\\begin{pmatrix} -0.29 &amp; -0.53 &amp; -0.19 &amp; 0.63 &amp; 0.22 &amp; 0.41 \\end{pmatrix}^T\\\\\\lambda_3=1.63,p_3=\\begin{pmatrix} -0.28 &amp; 0.75 &amp; -0.45 &amp; 0.20 &amp; -0.12 &amp; 0.33 \\end{pmatrix}^T\\\\\\lambda_4=1,p_4=\\begin{pmatrix} 0.00 &amp; -0.00 &amp; 0.58 &amp; -0.00 &amp; -0.58 &amp; 0.58 \\end{pmatrix}^T\\\\\\lambda_5=0.16,p_5=\\begin{pmatrix} 0.53 &amp; -0.29 &amp; -0.63 &amp; -0.19 &amp; -0.41 &amp; 0.22 \\end{pmatrix}^T\\\\\\lambda_6=0.16,p_6=\\begin{pmatrix} 0.00 &amp; -0.00 &amp; -0.00 &amp; -0.58 &amp; 0.58 &amp; 0.58 \\end{pmatrix}^T\\end{equation}$$ 则根据以上SVD分解有： $$\\begin{eqnarray}C&amp;=&amp;UΣV^T=\\begin{pmatrix}1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1\\end{pmatrix}\\\\&amp;=&amp;\\begin{pmatrix}0.44 &amp; 0.30 &amp; 0.57 &amp; 0.58 &amp; -0.25 \\\\0.13 &amp; 0.33 &amp; -0.59 &amp; -0.00 &amp; -0.73 \\\\0.48 &amp; 0.51 &amp; -0.37 &amp; -0.00 &amp; 0.61 \\\\0.70 &amp; -0.35 &amp; 0.15 &amp; -0.58 &amp; -0.16 \\\\0.26 &amp; -0.65 &amp; -0.41 &amp; 0.58 &amp; 0.09\\end{pmatrix}\\\\&amp;\\times&amp;\\begin{pmatrix}2.16 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 1.59 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1.28 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 1.00 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.39 &amp; 0\\end{pmatrix}\\\\&amp;\\times&amp;\\begin{pmatrix}-0.75 &amp; -0.29 &amp; -0.28 &amp; 0.00 &amp; 0.53 &amp; 0.00 \\\\-0.28 &amp; -0.53 &amp; 0.75 &amp; -0.00 &amp; -0.29 &amp; -0.00 \\\\-0.20 &amp; -0.19 &amp; -0.45 &amp; 0.58 &amp; -0.63 &amp; -0.00 \\\\-0.45 &amp; 0.63 &amp; 0.20 &amp; -0.00 &amp; -0.19 &amp; -0.58 \\\\-0.33 &amp; 0.22 &amp; -0.12 &amp; -0.58 &amp; -0.41 &amp; 0.58 \\\\-0.12 &amp; 0.41 &amp; 0.33 &amp; 0.58 &amp; 0.22 &amp; 0.58\\end{pmatrix} ^T\\\\&amp;=&amp;\\begin{pmatrix}-1.10 &amp; 0.06 &amp; -0.21 &amp; 0.04 &amp; -0.59 &amp; 0.62 \\\\-0.30 &amp; -0.84 &amp; 0.36 &amp; 0.11 &amp; 0.23 &amp; -0.13 \\\\-0.74 &amp; -1.14 &amp; -0.30 &amp; -0.09 &amp; -0.20 &amp; 0.11 \\\\-1.07 &amp; 0.04 &amp; -0.59 &amp; -0.98 &amp; -0.28 &amp; -0.69 \\\\0.04 &amp; -0.02 &amp; 0.62 &amp; -1.01 &amp; -0.69 &amp; -0.32\\end{pmatrix}\\end{eqnarray}$$ 取$k=2$，此时左奇异向量为 $$\\begin{eqnarray}U_{5\\times 2}=\\begin{pmatrix}0.44 &amp; 0.30 \\\\0.13 &amp; 0.33\\\\0.48 &amp; 0.51\\\\0.70 &amp; -0.35\\\\0.26 &amp; -0.65\\end{pmatrix}\\end{eqnarray}$$ 因为$0.44 \\gt 0.30$，这表示第一个词与第一维空间更接近，依次类推。 右奇异矩阵为： $$\\begin{eqnarray}U^T_{2\\times 6}=\\begin{pmatrix}-0.75 &amp; -0.28 &amp; -0.20 &amp; -0.45 &amp; -0.33 &amp; -0.12 \\\\-0.29 &amp; -0.53 &amp; -0.19 &amp; 0.63 &amp; 0.22 &amp; 0.41\\end{pmatrix}\\end{eqnarray}$$ 第一列表示文章$d_1$与第一维空间更接近，在些基础上可利用余弦相似度对两篇文档在低维空间上进行相似度计算。 中间矩阵为： $$\\begin{eqnarray}\\Sigma = \\begin{pmatrix}2.16 &amp; 0.00 \\\\0.00 &amp; 1.59\\end{pmatrix}\\end{eqnarray}$$ 表示的是词和文章的相关关系。 参考文献 奇异值分解 SVD矩阵分解及隐性语义索引《数学之美》拾遗——潜在语义索引(LSI)","raw":null,"content":null,"categories":[{"name":"ML","slug":"ML","permalink":"https://comwork2016.github.io/categories/ML/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://comwork2016.github.io/tags/NLP/"},{"name":"LSI","slug":"LSI","permalink":"https://comwork2016.github.io/tags/LSI/"}]},{"title":"奇异值分解 SVD","slug":"奇异值分解-SVD","date":"2017-04-12T00:56:22.000Z","updated":"2017-04-27T02:37:44.501Z","comments":true,"path":"2017/04/12/奇异值分解-SVD/","link":"","permalink":"https://comwork2016.github.io/2017/04/12/奇异值分解-SVD/","excerpt":"奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。","text":"奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。 线性变换的几何解释首先，我们来看一个只有两行两列的简单矩阵。 $$\\begin{equation}\\mathbf{M}=\\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2\\end{bmatrix}\\end{equation}$$ 从几何的角度，矩阵可以描述为一个变换：用矩阵乘法将平面上的点$(x,y)$ 变换成另外一个点 $(2x+y,x+2y)$ : $$\\begin{equation} \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 2x+y \\\\ x+2y \\end{bmatrix} \\nonumber\\end{equation}$$ 这种变换的效果如下： 不过这张图貌似也并没有能够简洁、清晰的描述出上述矩阵变换的几何效果。然而，如果我们把网格旋转45度，再观察一下。 我们看到现在这个新的网格在某一方向上被拉伸了3倍。 如果我们有一个2*2的对称矩阵，可以证明，我们总是可以通过在平面上旋转网格，使得矩阵变换的效果恰好是在两个垂直的方向上对网格的拉伸或镜面反射。 即给定一个对称矩阵 $M$ ，我们可以找到一组正交向量 $v_i$ 使得 $Mv_i$ 等于 $v_i$ 和标量的乘积；那就是 $$\\begin{equation} Mv_i = \\lambda_i v_i\\end{equation}$$ 这里$\\lambda_i$ 是标量。从几何意义上讲，这意味着当$v_i$ 乘上矩阵 $M$ 时被简单地拉伸或者反射了。因为这个性质，我们称 $v_i$ 是 $M$ 的特征向量；标量 $\\lambda_i$ 被称为特征值。一个可以被证明的重要的事实是：对称矩阵不同的特征值对应的特征向量是正交的。如果我们把对称矩阵的特征向量和网格对齐，那么矩阵对网格的拉伸或反射的方式，与矩阵对特征向量的拉伸或反射的方式，两者是完全一致的。 奇异值分解2*2矩阵奇异值分解的几何实质是：对于任意2*2矩阵，总能找到某个正交网格到另一个正交网格的转换与矩阵变换相对应。 用向量解释这个现象：选择适当的正交的单位向量 $v_1$ 和$v_2$ ，向量 $Mv_i$ 和 $Mv_2$ 也是正交的。 用 $u_1$ 和 $u_2$ 来表示 $Mv_1$ 和 $Mv_2$ 方向上的单位向量。 $Mv_1$ 和 $Mv_2$ 的长度用$\\sigma_1$ 和 $\\sigma_2$ 来表示——量化了网格在特定方向上被拉伸的效果。 $\\sigma_1$ 和 $\\sigma_2$ 被称为 $M$ 的奇异值。 由此，我们有 $$\\begin{equation} Mv_1 = \\sigma_1 u_1\\\\\\ Mv_2 = \\sigma_2 u_2\\end{equation}$$ 因为向量 $v_1$ 和 $v_2$ 是正交的单位向量，我们有 $$\\begin{equation} x = (v_1 \\cdot x)v_1 + (v_2 \\cdot x)v_2\\end{equation}$$ $v_1 \\cdot x$ 为单位向量与向量的内积，表示向量在该单位向量方向上的投影。 则有： $$\\begin{eqnarray} Mx &amp;=&amp; (v_1 \\cdot x)Mv_1 + (v_2 \\cdot x)Mv_2 \\\\ &amp;=&amp; (v_1 \\cdot x)\\sigma_1 u_1 + (v_2 \\cdot x)\\sigma_2 u_2\\end{eqnarray}$$ 注意点积（标量）可以用向量的转置来计算: $$\\begin{equation} v \\cdot x = v^T x\\end{equation}$$ 又有： $$\\begin{equation} Mx = u_1\\sigma_1 v_1^T x + u_2\\sigma_2 v_2^T x \\\\ M = u_1\\sigma_1 v_1^T + u_2\\sigma_2 v_2^T\\end{equation}$$ 即有：$$\\begin{eqnarray} M &amp;=&amp; \\left[\\begin{matrix} u_1 &amp; u_2 \\end{matrix}\\right] \\left[\\begin{matrix} \\sigma_1 &amp; \\\\ &amp;\\sigma_2 \\end{matrix}\\right] \\left[\\begin{matrix} v_1 &amp; v_2 \\end{matrix}\\right]^T \\\\ &amp;=&amp; \\left[\\begin{matrix} \\sigma_1u_1 &amp; \\sigma_2u_2 \\end{matrix}\\right] \\left[\\begin{matrix} v_1^T \\\\ v_2^T \\end{matrix}\\right] \\\\ &amp;=&amp; u_1\\sigma_1 v_1^T + u_2\\sigma_2 v_2^T\\end{eqnarray}$$ 将上式写成矩阵相乘的形式有： $$\\begin{equation} M = UΣV^T\\end{equation}$$ 这里 $U$ 是列向量 $u_1$ 和 $u_2$ 组成的矩阵，$Σ$ 是非零项为 $\\sigma_1$ 和 $\\sigma_2$ 的对角矩阵， $V$ 是列向量 $v_1$ 和 $v_2$ 组成的矩阵。 则有： $$\\begin{equation} MM^T=U(ΣΣ^T)U^T \\\\ M^T M = V(Σ^T Σ)V^T\\end{equation}$$ 关系式的右边描述了关系式左边的特征值分解。于是： $U$ 的列向量（左奇异向量）是 $MM^T$ 的特征向量。 $V$ 的列向量（右奇异向量）是 $M^T M$ 的特征向量。 $Σ$ 的非零对角元（非零奇异值）是 $MM^T$ 或者 $M^T M$ 的非零特征值的平方根。 上面描述了怎样将矩阵 $M$ 分解成三个矩阵的乘积： $V$ 描述了原始空间中的正交基， $U$ 描述了相关空间的正交基， $Σ$ 描述了 $V$ 中的向量变成 $U$ 中的向量时被拉伸的倍数。 举例SVD分解对以下矩阵进行SVD分解： $$\\begin{equation}C=\\begin{pmatrix}1 &amp; -1 \\\\0 &amp; 1\\\\1 &amp; 0\\\\-1 &amp; 1\\end{pmatrix}\\end{equation}$$ 计算$CC^T$矩阵的值如下： $$\\begin{equation}CC^T=\\begin{pmatrix}1 &amp; -1 \\\\0 &amp; 1\\\\1 &amp; 0\\\\-1 &amp; 1\\end{pmatrix}\\begin{pmatrix}1 &amp; 0 &amp; 1 &amp; -1\\\\-1 &amp; 1 &amp; 0 &amp; 1\\end{pmatrix}=\\begin{pmatrix}2 &amp; -1 &amp; 1 &amp; -2\\\\-1 &amp; 1 &amp; 0 &amp; 1\\\\1 &amp; 0 &amp; 1 &amp; -1\\\\-2 &amp; 1 &amp; -1 &amp; 2\\\\\\end{pmatrix} \\\\\\end{equation}$$ 对以上结果求解特征值和特征向量为： $$\\begin{equation}\\lambda_1 = 5,p_1=\\begin{pmatrix} -0.63 &amp; 0.32 &amp; -0.32 &amp; 0.63 \\end{pmatrix}^T\\\\\\lambda_2 = 1,p_2=\\begin{pmatrix} -0.00 &amp; -0.71 &amp; -0.71 &amp; 0.00 \\end{pmatrix}^T\\\\\\lambda_3 = 0,p_3=\\begin{pmatrix} -0.77 &amp; -0.26 &amp; 0.26 &amp; -0.52 \\end{pmatrix}^T\\\\\\lambda_4 = 0,p_4=\\begin{pmatrix} 0.02 &amp; -0.57 &amp; 0.57 &amp; 0.59 \\end{pmatrix}^T\\end{equation}$$ 计算$C^TC$矩阵的值如下： $$\\begin{equation}C^TC=\\begin{pmatrix}1 &amp; 0 &amp; 1 &amp; -1\\\\-1 &amp; 1 &amp; 0 &amp; 1\\end{pmatrix}\\begin{pmatrix}1 &amp; -1 \\\\0 &amp; 1\\\\1 &amp; 0\\\\-1 &amp; 1\\end{pmatrix}=\\begin{pmatrix}3 &amp; -2\\\\-2 &amp; 3\\end{pmatrix}\\end{equation}$$ 对以上结果求解特征值和特征向量为： $$\\begin{equation}\\lambda_1 = 5,p_1=\\begin{pmatrix} 0.71 &amp; -0.71 \\end{pmatrix}^T\\\\\\lambda_2 = 1,p_2=\\begin{pmatrix} 0.71 &amp; 0.71 \\end{pmatrix}^T\\end{equation}$$ 即，矩阵$C$分解之后有$$\\begin{equation}U=\\begin{pmatrix}-0.63 &amp; 0.00 \\\\0.32 &amp; -0.71 \\\\-0.32 &amp; -0.71 \\\\0.63 &amp; -0.00\\end{pmatrix}\\\\\\sqrt{S}=\\begin{pmatrix}2.236 &amp; 0\\\\0 &amp; 1\\end{pmatrix}\\\\V=\\begin{pmatrix}0.7071 &amp; 0.7071\\\\-0.7071 &amp; 0.7071\\end{pmatrix}\\end{equation}$$ 则有：$$\\begin{equation}C=U\\sqrt{S}V^T=\\begin{pmatrix}-0.63 &amp; 0.00 \\\\0.32 &amp; -0.71 \\\\-0.32 &amp; -0.71 \\\\0.63 &amp; -0.00\\end{pmatrix}\\begin{pmatrix}2.236 &amp; 0\\\\0 &amp; 1\\end{pmatrix}\\begin{pmatrix}0.7071 &amp; 0.7071\\\\-0.7071 &amp; 0.7071\\end{pmatrix}^T=\\begin{pmatrix}-1.00 &amp; 1.00 \\\\0.00 &amp; -1.00 \\\\-1.00 &amp; -0.00 \\\\1.00 &amp; -1.00\\end{pmatrix}\\end{equation}$$ 以上分解之后的矩阵的符号相反不影响。 数据压缩奇异值分解可以高效的表示数据。例如，假设我们想传送下列图片，包含15*25个黑色或者白色的像素阵列。 因为在图像中只有三种类型的列（如下），它可以以更紧凑的形式被表示。 我们用15*25的矩阵来表示这个图像，其中每个元素非0即1，0表示黑色像素，1表示白色像素。如下所示，共有375个元素。 如果对M进行奇异值分解的话，我们只会得到三个非零的奇异值。 $$\\begin{equation} \\sigma_1 = 14.72 \\\\ \\sigma_2 = 5.22 \\\\ \\sigma_3 = 3.31 \\\\\\end{equation}$$ 因此，矩阵可以如下表示 $$\\begin{equation} M = u_1\\sigma_1 v_1^T + u_2\\sigma_2 v_2^T + u_3\\sigma_3 v_3^T\\end{equation}$$ 我们有三个包含15个元素的向量 $v_i$ ，三个包含25个元素的向量 $u_i$ ，以及三个奇异值 $\\sigma_i$ 。这意味着我们可以只用123个数字就能表示这个矩阵而不是出现在矩阵中的375个元素。在这种方式下，我们看到在矩阵中有3个线性独立的列，也就是说矩阵的秩是3。 参考文献 奇异值分解（We Recommend a Singular Value Decomposition）","raw":null,"content":null,"categories":[{"name":"ML","slug":"ML","permalink":"https://comwork2016.github.io/categories/ML/"}],"tags":[{"name":"math","slug":"math","permalink":"https://comwork2016.github.io/tags/math/"},{"name":"SVD","slug":"SVD","permalink":"https://comwork2016.github.io/tags/SVD/"}]},{"title":"最大公约数（辗转相除法）","slug":"最大公约数（辗转相除法）","date":"2017-04-09T08:06:44.000Z","updated":"2017-04-09T08:33:04.463Z","comments":true,"path":"2017/04/09/最大公约数（辗转相除法）/","link":"","permalink":"https://comwork2016.github.io/2017/04/09/最大公约数（辗转相除法）/","excerpt":"辗转相除法，又称欧几里得算法，是求最大公约数(Greatest Common Divisor)的算法。","text":"辗转相除法，又称欧几里得算法，是求最大公约数(Greatest Common Divisor)的算法。 算法描述设两数为$a$、$b$ $(a&gt;b)$，求$a$和$b$最大公约数$gcd(a，b)$的步骤如下： 用$b$除$a$，得$a÷b=q……r(0\\leq r)$。 若$r=0$，则$gcd(a,b)=b$；结束。 若$r≠0$，取$a=b,b=r$，执行第1步。 原理证明设两数为$a$、$b$ $(b\\leq a)$，用$gcd(a,b)$表示$a$，$b$的最大公约数，$r=a\\ mod\\ b $为$a$除以$b$以后的余数，$k$为$a$除以$b$的商，即$a÷b=k…….r$。 辗转相除法即是要证明$gcd(a,b)=gcd(b,r)$。 令$c=gcd(a,b)$，则设$a=mc$，$b=nc$ 则$r =a-kb=mc-knc=(m-kn)c$ 即$c$也是$r$的因数 可以断定$m-kn$与$n$互素【否则，可设$m-kn=xd$，$n=yd$，$(d&gt;1)$，则$m=kn+xd=kyd+xd=(ky+x)d$，则$a=mc=(ky+x)dc$，$b=nc=ycd$，故$a$与$b$最大公约数成为$cd$，而非$c$，与前面结论矛盾】 从而可知$gcd(b,r)=c$，继而$gcd(a,b)=gcd(b,r)$。 算法实现(c++)递归方式：1234567int gcd(int a,int b)&#123; if(b == 0) return b; else return gcd(b, a % b)&#125; 迭代方式：12345678910int gcd(int a, int b)&#123; while(b != 0) &#123; int r = a % b; a = b; b = r; &#125; return a;&#125;","raw":null,"content":null,"categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://comwork2016.github.io/categories/algorithm/"}],"tags":[{"name":"math","slug":"math","permalink":"https://comwork2016.github.io/tags/math/"},{"name":"gcd","slug":"gcd","permalink":"https://comwork2016.github.io/tags/gcd/"}]},{"title":"Logistic Regression","slug":"Logistic-Regression","date":"2017-04-01T01:12:45.000Z","updated":"2017-04-27T02:37:57.694Z","comments":true,"path":"2017/04/01/Logistic-Regression/","link":"","permalink":"https://comwork2016.github.io/2017/04/01/Logistic-Regression/","excerpt":"本文主要参考Andrew Ng老师的Machine Learning公开课，并用《机器学习实战》中的源码实现。","text":"本文主要参考Andrew Ng老师的Machine Learning公开课，并用《机器学习实战》中的源码实现。 Logistic Regression基本原理Logistic分布Logistic Distribution的分布函数和密度函数如下： $$\\begin{equation}F(x) = P(X \\leqslant x) = \\frac{1}{1+e^{-(x-\\mu)/\\gamma}}\\end{equation}$$ $$\\begin{equation}f(x)=F’(x) = \\frac{e^{-(x-\\mu)/\\gamma}} { \\gamma (1+e^{-(x-\\mu)/\\gamma})^2 }\\end{equation}$$ 上式中$ \\mu $是位置参数，$ \\gamma &gt; 0 $是形状参数。下图是不同参数对logistic分布的影响，从图中可以看到可以看到 $ \\mu $ 影响的是中心对称点的位置，$ \\gamma $越小中心点附近增长的速度越快。而常常在深度学习中用到的非线性变换$ sigmoid $函数是逻辑斯蒂分布的$ \\gamma=1,\\mu=0 $的特殊形式。 二项Logistic Regression模型 逻辑回归是为了解决分类问题，根据一些已知的训练集训练好模型，再对新的数据进行预测属于哪个类。对于上图中的数据，逻辑回归的目标是找到一个有足够好区分度的决策边界，从而能够将两类很好的分开。 分离边界的维数与空间的维数相关。如果是二维平面，分离边界就是一条线（一维）。如果是三维空间，分离边界就是一个空间中的面（二维）。如果是一维直线，分离边界就是直线上的某一点。 假设输入的特征向量为$ x \\in R^n $，$ Y $取值为$ 0，1 $。对于二维的空间，决策边界可以表示为$ w_1x_1+w_2x_2+b=0 $，假如存在一个例子使得$ h_w(x)=w_1x_1+w_2x_2+b&gt;0 $，那么可以判断它类别为$ 1 $，这个过程实际上是感知机，即只通过决策函数的符号来判断属于哪一类。而逻辑回归需要再进一步，它要找到分类概率$ P(Y=1)$与输入向量$ x $的直接关系，然后通过比较概率值来判断类别，而刚好上文中的logistic function能满足这样的要求，它令决策函数的输出值$ w^Tx+b = log \\frac{P(Y=1|x)}{1−P(Y=1|x)} $，求解这个式子得到了输入向量$ x $下导致产生两类的概率为： $$\\begin{equation}P(Y=1|x)=\\frac{e^{w\\cdot x+b}}{1+e^{w\\cdot x+b}}\\label{eq:logistic1}\\end{equation}$$ $$\\begin{equation}P(Y=0|x)=\\frac{1}{1+e^{w\\cdot x+b}}\\end{equation}$$ 其中$ w $称为权重，$ b $称为偏置，其中的$ w⋅x+b $看成对$ x $的线性函数，有时候为了书写方便，会将$ b $写入$ w $，即 $ w=(b,w_1,…,w_n) $ ，并取$ x=(1,x_1,…,x_n) $。然后对比上面两个概率值，概率值大的就是$ x $对应的类。 又已知一个事件发生的几率odds是指该事件发生与不发生的概率比值，二分类情况下即$ \\frac{P(Y=1|x)}{P(Y=0|x)}=\\frac{P(Y=1|x)}{1−P(Y=1|x)} $。取odds的对数就是上面提到的logistic function，$ logistic(P(Y=1|x))=log\\frac{P(Y=1|x)}{1−P(Y=1|x)}=w⋅x $。从而可以得到一种对逻辑回归的定义，输出$ Y=1 $的对数几率是由输入$ x $的线性函数表示的模型，即逻辑斯蒂回归模型(李航.《统计机器学习》)。而直接考察公式$\\eqref{eq:logistic1}$可以得到另一种对逻辑回归的定义，线性函数的值越接近正无穷，概率值就越接近1；线性值越接近负无穷，概率值越接近0，这样的模型是逻辑斯蒂回归模型(李航.《统计机器学习》)。因此逻辑回归的思路是，先拟合决策边界(这里的决策边界不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。 有了上面的分类概率，就可以建立似然函数，通过极大似然估计法来确定模型的参数。设$ P(Y=1|x)=h_w(x) $，似然函数为$ \\prod [h_w(x^{(i)})]^{y^{(i)}}[1-h_w(x^{(i)})]^{(1-y^{(i)})} $，对数似然函数为 $$\\begin{eqnarray}L(w) &amp; = &amp; \\sum_{i=1}^{N}\\log P(y^{(i)}|x^{(i)};w) \\\\&amp; = &amp; \\sum_{i=1}^{N}[y^{(i)}\\log h_w(x^{(i)})+(1-y^{(i)})\\log(1-h_w(x^{(i)}))]\\end{eqnarray}$$ 优化方法优化的主要目标是找到一个方向，参数朝这个方向移动之后使得似然函数的值能够减小，这个方向往往由一阶偏导或者二阶偏导各种组合求得。逻辑回归的损失函数是 $$\\begin{eqnarray}min J(w) &amp;=&amp; \\min \\frac{1}{m} \\sum_{j=1}^{m}Cost(h_w(x^{(i)}),y^{(i)}) \\\\&amp;=&amp; \\min {-\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}\\log h_w(x^{(i)})+(1-y^{(i)})\\log(1-h_w(x^{(i)}))]}\\end{eqnarray}$$ 梯度下降法 最大似然估计就是要求得使$ J(θ) $取最大值时的$ θ $，但因此处的$ Cost(h_w(x^{(i)}),y^{(i)}) $添加了一个负号，所以必须用梯度下降法求解最佳参数。但若此处的$ Cost(h_w(x^{(i)}),y^{(i)}) $没有添加负号，则需要用梯度上升法求解最佳参数。 先把$ J(w) $对$ w_j $的一阶偏导求出来，且用$ g $表示。$ g $是梯度向量。 $$\\begin{eqnarray}g_j &amp;=&amp; \\frac{\\partial J(w)}{\\partial w_j}\\\\&amp;=&amp; -\\frac{1}{m}\\sum_{i=1}^{m}(\\frac{y^{(i)}}{h_w(x^{(i)})} h_w(x^{(i)}) (1-h_w(x^{(i)}))(-x_{j}^{(i)}) + (1-y^{(i)})\\frac {1}{1-h_w(x^{(i)})}h_w(x^{(i)})(1-h_w(x^{(i)}))x_j^{(i)}) \\\\&amp;=&amp; -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}-h_w(x^{(i)}))x_{j}^{(i)})\\end{eqnarray}$$ 梯度下降是通过$ J(w) $对$ w $的一阶导数来找下降方向（负梯度），并且以迭代的方式来更新参数，更新方式为 $$\\begin{eqnarray}w^{k+1}_j &amp;=&amp; w^k_j+α(-g_j)\\\\&amp;=&amp;w^k_j+α \\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}-h_w(x^{(i)}))x_{j}^{(i)}\\label{eq:lr-gd}\\end{eqnarray}$$ $ k $为迭代次数。每次更新参数后，可以通过比较$||J(w^{k+1})−J(w^k)||$或者$ ||w^{k+1}−w^k ||$与某个阈值$ \\epsilon $大小的方式来停止迭代，即比阈值小就停止。 如果采用梯度上升法来推到参数的更新方式，会发现式子与公式$\\eqref{eq:lr-gd}$完全一样，所以采用梯度上升发和梯度下降法是一样的。 随机梯度下降法从上面梯度下降法中的公式$\\eqref{eq:lr-gd}$中可以看到，每次更新回归系数时都需要遍历整个数据集，如果有数十亿样本和成千上万个特征，则梯度下降法的计算复杂度就太高了。随机梯度下降法一次仅用一个样本点来更新回归系数： $$\\begin{equation}w^{k+1}_j = w^k_j+α (y^{(i)}-h_w(x^{(i)}))x_{j}^{(i)}\\end{equation}$$ 梯度下降过程向量化约定训练数据的矩阵形式如下，$ x $的每一行为一条训练样本，而每一列为不同的特称取值： $$\\begin{equation}x=\\left[\\begin{matrix}x^{(1)}\\\\x^{(2)}\\\\\\ldots\\\\x^{(m)}\\end{matrix}\\right]=\\left[\\begin{matrix}x_0^{(1)} &amp; x_1^{(1)} &amp; \\ldots &amp; x_n^{(1)}\\\\x_0^{(2)} &amp; x_1^{(2)} &amp; \\ldots &amp; x_n^{(2)}\\\\\\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\x_0^{(m)} &amp; x_1^{(m)} &amp; \\ldots &amp; x_n^{(m)}\\end{matrix}\\right],y=\\left[\\begin{matrix}y^{(1)}\\\\y^{(2)}\\\\\\ldots\\\\y^{(m)}\\end{matrix}\\right]\\end{equation}$$ 约定待求的参数θ的矩阵形式为： $$\\begin{equation}\\theta =\\left[\\begin{matrix}\\theta_1\\\\\\theta_2\\\\\\ldots\\\\\\theta_n\\end{matrix}\\right]\\end{equation}$$ 先求$ x \\cdot \\theta $并记为$ A $： $$\\begin{equation}A=x \\cdot \\theta=\\left[\\begin{matrix}x_0^{(1)} &amp; x_1^{(1)} &amp; \\ldots &amp; x_n^{(1)}\\\\x_0^{(2)} &amp; x_1^{(2)} &amp; \\ldots &amp; x_n^{(2)}\\\\\\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\x_0^{(m)} &amp; x_1^{(m)} &amp; \\ldots &amp; x_n^{(m)}\\end{matrix}\\right]\\cdot\\left[\\begin{matrix}\\theta_0\\\\\\theta_1\\\\\\ldots\\\\\\theta_n\\end{matrix}\\right]=\\left[\\begin{matrix}\\theta_0x_0^{(1)} + \\theta_1x_1^{(1)} + \\ldots + \\theta_nx_n^{(1)}\\\\\\theta_0x_0^{(2)} + \\theta_1x_1^{(2)} + \\ldots + \\theta_nx_n^{(2)}\\\\\\ldots \\\\\\theta_0x_0^{(m)} + \\theta_1x_1^{(m)} + \\ldots + \\theta_nx_n^{(m)}\\end{matrix}\\right]\\end{equation}$$ 求$ h_\\theta(x)-y $并记为$ E $： $$\\begin{equation}E=h_\\theta(x)-y=\\left[\\begin{matrix}g(A^{(1)})-y^{(1)}\\\\g(A^{(2)})-y^{(2)}\\\\\\ldots \\\\g(A^{(m)})-y^{(m)}\\end{matrix}\\right]=\\left[\\begin{matrix}e^{(1)}\\\\e^{(2)}\\\\\\ldots\\\\e^{(m)}\\end{matrix}\\right]=g(A)-y\\end{equation}$$ 由上式可知$ h_\\theta(x)-y $可以由$ g(A)-y $一次计算求得。 再来看一下公式$\\eqref{eq:lr-gd}$的$\\theta$更新过程： $$\\begin{eqnarray}\\theta_j &amp;=&amp; \\theta_j + \\alpha \\sum_{i=1}^{m}(-e^{(i)})x_j^{(i)}\\\\&amp;=&amp; \\theta_j-\\alpha\\cdot(x_j^{(1)},x_j^{(2)},\\ldots,x_j^{(m)})\\cdot E\\end{eqnarray}$$ 综合上面的式子有： $$\\begin{equation}\\theta = \\theta - \\alpha\\cdot\\frac{1}{m}\\cdot x^T\\cdot(g(x\\cdot\\theta)-y)\\end{equation}$$ 正则化由于模型的参数个数一般是由人为指定和调节的，所以正则化常常是用来限制模型参数值不要过大，也被称为惩罚项。一般是在目标函数(经验风险)中加上一个正则化项$ \\Phi(w) $即 $$\\begin{equation}J(w) = -\\frac{1}{m}[\\sum_{i=1}^{m}y_ilog h_w (x_i) + (1-y_i)log(1-h_w(x_i))] + \\lambda \\Phi(w)\\label{eq:reg}\\end{equation}$$ 而这个正则化项一般会采用L1范数或者L2范数。其形式分别为$ \\Phi (w)=||x||_1 $和$ \\Phi (w)=||x||_2 $。 首先针对L1范数$ \\Phi (w)=|x| $，当采用梯度下降方式来优化目标函数时，对目标函数进行求导，正则化项导致的梯度变化当$ w_j &gt; 0 $是取1，当$ w_j &lt; 0 $时取-1. 从而导致的参数$w_j$减去了学习率与公式的乘积，因此当$ w_j &gt; 0 $的时候，$ w_j$会减去一个正数，导致$ w_j $减小，而当$ w_j &lt; 0 $的时候，$ w_j$会减去一个负数，导致$ w_j$又变大，因此这个正则项会导致参数$ w_j$取值趋近于0，也就是为什么L1正则能够使权重稀疏，这样参数值就受到控制会趋近于0。L1正则还被称为 Lasso regularization。 然后针对L2范数$\\phi(w) = \\sum_{j=1}^{n}w_j^2$，同样对它求导，得到梯度变化为$\\frac{\\partial \\Phi(w)}{\\partial w_j} = 2w_j$(一般会用$\\frac{\\lambda}{2}$来把这个系数2给消掉)。同样的更新之后使得$ w_j$的值不会变得特别大。在机器学习中也将L2正则称为weight decay，在回归问题中，关于L2正则的回归还被称为Ridge Regression岭回归。weight decay还有一个好处，它使得目标函数变为凸函数，梯度下降法和L-BFGS都能收敛到全局最优解。 需要注意的是，L1正则化会导致参数值变为0，但是L2却只会使得参数值减小，这是因为L1的导数是固定的，参数值每次的改变量是固定的，而L2会由于自己变小改变量也变小。而公式$\\eqref{eq:reg}$中的$\\lambda$也有着很重要的作用，它在权衡拟合能力和泛化能力对整个模型的影响，$\\lambda$越大，对参数值惩罚越大，泛化能力越好。 《机器学习实战》代码梯度上升法： 1234567891011121314def gradAscent(dataMatIn, classLabels): \"\"\"梯度上升法\"\"\" dataMatrix = mat(dataMatIn) labelMat = mat(classLabels).transpose() m, n = shape(dataMatrix) alpha = 0.1 maxCycles = 500 weights = ones((n, 1)) for k in range(maxCycles): a = dataMatrix * weights h = sigmoid(dataMatrix * weights) # 100*3 3*1 error = (labelMat - h) weights = weights + alpha / m * dataMatrix.transpose() * error return weights 随机梯度下降法： 1234567891011121314151617181920212223242526def stocGradAscent0(dataMatrix, classLabels): \"\"\"随机梯度上升法，但是迭代次数不够，且可能存在局部波动现象\"\"\" m, n = shape(dataMatrix) alpha = 0.01 weights = ones(n) for i in range(m): h = sigmoid(sum(dataMatrix[i] * weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] return weightsdef stocGradAscent1(dataMatrix, classLabels, numIter=150): \"\"\"改进的随机梯度上升法\"\"\" m, n = dataMatrix.shape weights = ones(n) for j in range(numIter): dataIndex = range(m) for i in range(m): alpha = 4 / (1.0 + j + i) + 0.01 # alpha在每次迭代时都进行了调整 randIndex = int(random.uniform(0, len(dataIndex))) # 随机选取样本数据 h = sigmoid(sum(dataMatrix[randIndex] * weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] del (dataIndex[randIndex]) return weights 参考文献 【机器学习笔记1】Logistic回归总结【机器学习算法系列之二】浅析Logistic Regression牛顿法与拟牛顿法学习笔记（一）牛顿法 MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\" } } });","raw":null,"content":null,"categories":[{"name":"ML","slug":"ML","permalink":"https://comwork2016.github.io/categories/ML/"}],"tags":[{"name":"LR","slug":"LR","permalink":"https://comwork2016.github.io/tags/LR/"}]},{"title":"Latex数学公式","slug":"Latex数学公式","date":"2017-04-01T00:42:07.000Z","updated":"2017-05-18T01:40:04.631Z","comments":true,"path":"2017/04/01/Latex数学公式/","link":"","permalink":"https://comwork2016.github.io/2017/04/01/Latex数学公式/","excerpt":"Latex中的常用命令。","text":"Latex中的常用命令。 多行的数学公式1234\\begin&#123;eqnarray*&#125;\\cos 2\\theta &amp; = &amp; \\cos^2 \\theta - \\sin^2 \\theta \\\\&amp; = &amp; 2 \\cos^2 \\theta - 1.\\end&#123;eqnarray*&#125; 其中&amp;是对其点，表示在此对齐。$$\\begin{eqnarray*}\\cos 2\\theta &amp; = &amp; \\cos^2 \\theta - \\sin^2 \\theta \\\\&amp; = &amp; 2 \\cos^2 \\theta - 1.\\end{eqnarray*}$$ 数学公式自动编号在文件中添加一下javascript代码12345&lt;script type=\"text/x-mathjax-config\"&gt;MathJax.Hub.Config(&#123; TeX: &#123; equationNumbers: &#123; autoNumber: \"AMS\" &#125; &#125;&#125;);&lt;/script&gt; 公式的标记和引用使用：\\label{maker}来标记公式，使用\\eqref{maker}来引用公式。 1234\\begin&#123;equation&#125;\\cos 2\\theta = \\cos^2 \\theta - \\sin^2 \\theta\\label&#123;eq:test1&#125;\\end&#123;equation&#125; $$\\begin{equation}\\cos 2\\theta = \\cos^2 \\theta - \\sin^2 \\theta\\label{eq:test1}\\end{equation}$$如引用公式$\\ref{eq:test1}$ MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\" } } });","raw":null,"content":null,"categories":[{"name":"tools","slug":"tools","permalink":"https://comwork2016.github.io/categories/tools/"}],"tags":[{"name":"math","slug":"math","permalink":"https://comwork2016.github.io/tags/math/"},{"name":"latex","slug":"latex","permalink":"https://comwork2016.github.io/tags/latex/"}]},{"title":"删除已经提交的Ignore file","slug":"删除已经提交的Ignore-file","date":"2017-03-31T10:55:30.000Z","updated":"2017-04-01T11:56:30.616Z","comments":true,"path":"2017/03/31/删除已经提交的Ignore-file/","link":"","permalink":"https://comwork2016.github.io/2017/03/31/删除已经提交的Ignore-file/","excerpt":"","text":"To untrack a single file that has already been added/initialized to your repository, i.e., stop tracking the file but not delete it from your system use: 1git rm --cached filename To untrack every file that is now in your .gitignore: First commit any outstanding code changes, and then, run this command: 1git rm -r --cached . This removes any changed files from the index(staging area), then just run: 1git add . Commit it: 1git commit -m \".gitignore is now working\"","raw":null,"content":null,"categories":[{"name":"git","slug":"git","permalink":"https://comwork2016.github.io/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://comwork2016.github.io/tags/git/"},{"name":"ignore","slug":"ignore","permalink":"https://comwork2016.github.io/tags/ignore/"}]},{"title":"用rebase -i 修改提交","slug":"用rebase-i-修改提交","date":"2017-03-31T10:25:26.000Z","updated":"2017-04-01T11:56:30.617Z","comments":true,"path":"2017/03/31/用rebase-i-修改提交/","link":"","permalink":"https://comwork2016.github.io/2017/03/31/用rebase-i-修改提交/","excerpt":"如果commit的message填写错误，可以通过git的rebase来修改提交信息。","text":"如果commit的message填写错误，可以通过git的rebase来修改提交信息。 用rebase -i，首先选择要修改的提交。 1$ git rebase -i HEAD~~ 打开文本编辑器，将看到从HEAD到HEAD~~的提交如下图显示。12345678910111213141516pick 9a54fd4 添加commit的说明pick 0d4a808 添加pull的说明# Rebase 326fc9f..0d4a808 onto d286baa## Commands:# p, pick = use commit# r, reword = use commit, but edit the commit message# e, edit = use commit, but stop for amending# s, squash = use commit, but meld into previous commit# f, fixup = like \"squash\", but discard this commit's log message# x, exec = run command (the rest of the line) using shell## If you remove a line here THAT COMMIT WILL BE LOST.# However, if you remove everything, the rebase will be aborted.# 将第一行的pick改成edit，然后保存并退出。将会显示以下内容，修改过的提交呈现退出状态。 12345678Stopped at d286baa... 添加commit的说明You can amend the commit now, with git commit --amendOnce you are satisfied with your changes, run git rebase --continue 打开sample.txt，适当地修改commit的讲解部分。 1234连猴子都懂的Git命令add 把变更录入到索引中commit 记录索引的状态pull 取得远端数据库的内容 用commit --amend保存修改。 12$ git add sample.txt$ git commit --amend 现在已经commit，但是rebase操作还没结束。若要通知这个提交的操作已经结束，请指定 --continue选项执行rebase。 1$ git rebase --continue 这时，有可能其他提交会发生冲突, 请修改冲突部分后再执行add和rebase –continue。这时不需要提交。如果在中途要停止rebase操作，请在rebase指定–abort选项执行，这样就可以抹去并停止在rebase的操作。 提交的修改完成了。如果要把多个提交修改成edit，下一个要修改的提交会退出，请执行同样的修改。 实际上，在rebase之前的提交会以ORIG_HEAD之名存留。如果rebase之后无法复原到原先的状态，可以用git reset –hard ORIG_HEAD复原到rebase之前的状态。","raw":null,"content":null,"categories":[{"name":"git","slug":"git","permalink":"https://comwork2016.github.io/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://comwork2016.github.io/tags/git/"}]},{"title":"windows 删除快捷方式箭头","slug":"windows-删除快捷方式箭头","date":"2017-03-31T10:14:52.000Z","updated":"2017-04-27T02:38:33.786Z","comments":true,"path":"2017/03/31/windows-删除快捷方式箭头/","link":"","permalink":"https://comwork2016.github.io/2017/03/31/windows-删除快捷方式箭头/","excerpt":"windows 删除快捷方式箭头的脚本。","text":"windows 删除快捷方式箭头的脚本。 删除快捷方式的箭头：1234567reg add \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Icons\" /v 29 /d \"%systemroot%\\system32\\imageres.dll,197\" /t reg_sz /ftaskkill /f /im explorer.exeattrib -s -r -h \"%userprofile%\\AppData\\Local\\iconcache.db\"del \"%userprofile%\\AppData\\Local\\iconcache.db\" /f /qstart explorerpause` 同样，如果想恢复快捷方式小箭头，只需要将文本文件的内容改成以下内容即可：123456reg delete \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Icons\" /v 29 /ftaskkill /f /im explorer.exeattrib -s -r -h \"%userprofile%\\AppData\\Local\\iconcache.db\"del \"%userprofile%\\AppData\\Local\\iconcache.db\" /f /qstart explorerpause 然后再次以管理员身份运行即可。","raw":null,"content":null,"categories":[{"name":"tools","slug":"tools","permalink":"https://comwork2016.github.io/categories/tools/"}],"tags":[{"name":"windows","slug":"windows","permalink":"https://comwork2016.github.io/tags/windows/"},{"name":"快捷方式","slug":"快捷方式","permalink":"https://comwork2016.github.io/tags/快捷方式/"}]},{"title":"文件名过长，导致无法删除","slug":"文件名过长，导致无法删除","date":"2017-03-31T10:04:58.000Z","updated":"2017-04-01T11:56:30.617Z","comments":true,"path":"2017/03/31/文件名过长，导致无法删除/","link":"","permalink":"https://comwork2016.github.io/2017/03/31/文件名过长，导致无法删除/","excerpt":"文件名过长，导致无法删除的解决方案。","text":"文件名过长，导致无法删除的解决方案。 在桌面上新建“文本文档”（用附件中的“记事本”） 写入下列命令： 12DEL /F /A /Q \\\\?\\%1RD /S /Q \\\\?\\%1 另存为”删除OK.bat” 一定要选另存为！保存类型选“所有文件” 建好后，把要删除的文件或者目录拖放到这个bat文件的图标上就可以删除了。一切OK!","raw":null,"content":null,"categories":[{"name":"tools","slug":"tools","permalink":"https://comwork2016.github.io/categories/tools/"}],"tags":[{"name":"windows","slug":"windows","permalink":"https://comwork2016.github.io/tags/windows/"},{"name":"删除","slug":"删除","permalink":"https://comwork2016.github.io/tags/删除/"}]},{"title":"vi 颜色设置","slug":"vi-颜色设置","date":"2017-03-31T09:18:12.000Z","updated":"2017-04-27T02:38:22.593Z","comments":true,"path":"2017/03/31/vi-颜色设置/","link":"","permalink":"https://comwork2016.github.io/2017/03/31/vi-颜色设置/","excerpt":"编辑~./vimrc文件","text":"编辑~./vimrc文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667set nocompatible \" Use Vim defaults (much better!) set bs=2 \" allow backspacing over everything in insert mode set ai \" always set autoindenting on \"set backup \" keep a backup file set viminfo='20,\\\"50 \" read/write a .viminfo file, don't store more \" than 50 lines of registers set history=50 \" keep 50 lines of command line history set ruler \" show the cursor position all the time syntax on set hlsearch set incsearch set tabstop=4 set autoindent set cindent set confirm set number set expandtab set autoindent set smartindent filetype indent on if v:lang =~ \"utf8\" || v:lang =~ \"UTF-8\" set fileencodings=utf-8,latin1 endif set syn=cpp \" Only do this part when compiled with support for autocommands if has(\"autocmd\") \" In text files, always limit the width of text to 78 characters autocmd BufRead *.txt set tw=78 \" When editing a file, always jump to the last cursor position autocmd BufReadPost * \\ if line(\"'\\\"\") &gt; 0 &amp;&amp; line (\"'\\\"\") &lt;= line(\"&#123;1\") | \\ exe \"normal! g'\\\"\" | \\ endif endif if has(\"cscope\") set csprg=/usr/bin/cscope set csto=0 set cst set nocsverb \" add any database in current directory if filereadable(\"cscope.out\") cs add cscope.out \" else add database pointed to by environment elseif $CSCOPE_DB != \"\" cs add $CSCOPE_DB endif set csverb endif \" Switch syntax highlighting on, when the terminal has colors \" Also switch on highlighting the last used search pattern. syntax on set hlsearch if &amp;term==\"xterm\" set t_Co=8 set t_Sb=m set t_Sf=m endif set tags=tags se cursorline se cursorcolumn hi cursorline cterm=NONE ctermbg=darkred ctermfg=white guibg=darkred guifg=white hi cursorcolumn cterm=NONE ctermbg=darkred ctermfg=white guibg=darkred guifg=white","raw":null,"content":null,"categories":[{"name":"tools","slug":"tools","permalink":"https://comwork2016.github.io/categories/tools/"}],"tags":[{"name":"配色","slug":"配色","permalink":"https://comwork2016.github.io/tags/配色/"},{"name":"vi","slug":"vi","permalink":"https://comwork2016.github.io/tags/vi/"}]},{"title":"ubuntu终端颜色配置","slug":"ubuntu终端颜色配置","date":"2017-03-31T08:53:20.000Z","updated":"2017-04-27T02:38:42.900Z","comments":true,"path":"2017/03/31/ubuntu终端颜色配置/","link":"","permalink":"https://comwork2016.github.io/2017/03/31/ubuntu终端颜色配置/","excerpt":"打开终端（ctrl+alt+T），会发现里面都是一个颜色，不管是用户名、主机名还是命令都是白色，对开发人员来说带来了不便之处。因为有时候你需要去几十行甚至上百行代码里面去找一些你运行过的命令或你想要的信息。","text":"打开终端（ctrl+alt+T），会发现里面都是一个颜色，不管是用户名、主机名还是命令都是白色，对开发人员来说带来了不便之处。因为有时候你需要去几十行甚至上百行代码里面去找一些你运行过的命令或你想要的信息。 首先在终端里面用 gedit 打开配置文件（~/.bashrc），如： 1$ gedit ~/.bashrc 在最后添加如下代码： 1PS1='\\[\\033[1;31;40m\\]\\u@\\h:\\w\\$\\[\\033[00m\\] ' 重启终端，你就可以看到你的配色了，或者执行 source .bashrc 命令也可以运行新的配色。 配色过程： 前景 背景 颜色 30 40 黑色 31 41 紅色 32 42 绿色 33 43 黄色 34 44 蓝色 35 45 紫红色 36 46 青蓝色 37 47 白色 代码 意义 0 OFF 1 高亮显示 4 underline 5 闪烁 7 反白显示 8 不可见 一个单独的颜色设置:\\033[代码;前景;背景m，如：\\[\\033[1;32;40m\\]表示高亮显示字体为绿色，背景色为黑色。 注意：颜色的设置，放在相应的要设置的前面，如用户名颜色设置：\\[\\033[01;35;40m\\]\\u","raw":null,"content":null,"categories":[{"name":"tools","slug":"tools","permalink":"https://comwork2016.github.io/categories/tools/"}],"tags":[{"name":"bash","slug":"bash","permalink":"https://comwork2016.github.io/tags/bash/"},{"name":"配色","slug":"配色","permalink":"https://comwork2016.github.io/tags/配色/"}]},{"title":"VC6.0不能停止调试程序的解决方案","slug":"VC6-0不能停止调试程序的解决方案","date":"2017-03-31T08:45:09.000Z","updated":"2017-04-27T02:38:50.740Z","comments":true,"path":"2017/03/31/VC6-0不能停止调试程序的解决方案/","link":"","permalink":"https://comwork2016.github.io/2017/03/31/VC6-0不能停止调试程序的解决方案/","excerpt":"VC6.0在Windows7下调试的时候，再结束调试，程序无法退出。","text":"VC6.0在Windows7下调试的时候，再结束调试，程序无法退出。 问题描述 ：当我击F5开始一个项目的调试时，程序在我设置的断点处停止，这时按下Shift+F5后，VC6.0可以退出调试状态，但是windows系统的任务栏上会留下前面调试时产生的程序。该进程不能被结束，即使我使用任务管理器也不可以终止程序。而且，当修改代码之后，就不能重新编译了。想结束该进程的唯一的办法是关闭VC6.0，并重新开启。 解决方案 ：更新两个dll文件的版本。 在VC6.0安装目录下的 Common/MSDev98/Bin 里有两个dll文件：DM.dll 和 TLLOC.dll。将DM.dll替换成6.0.9782.0版本的或更新，将TLLOC.dll替换成6.00.8168.0版本的或更新。 C语言中文网提供了这两个dll文件的下载地址：http://pan.baidu.com/s/1eQHzLwm 提取密码：sg07","raw":null,"content":null,"categories":[{"name":"tools","slug":"tools","permalink":"https://comwork2016.github.io/categories/tools/"}],"tags":[{"name":"vc6.0","slug":"vc6-0","permalink":"https://comwork2016.github.io/tags/vc6-0/"}]},{"title":"intellij idea 注册服务器","slug":"intellij-idea-注册服务器","date":"2017-03-31T08:32:32.000Z","updated":"2017-05-05T02:51:38.082Z","comments":true,"path":"2017/03/31/intellij-idea-注册服务器/","link":"","permalink":"https://comwork2016.github.io/2017/03/31/intellij-idea-注册服务器/","excerpt":"","text":"Intellij idea 注册服务器地址 lisense server address: http://idea.iteblog.com/key.php 服务器更新地址:http://idea.lanyus.com/","raw":null,"content":null,"categories":[{"name":"tools","slug":"tools","permalink":"https://comwork2016.github.io/categories/tools/"}],"tags":[{"name":"idea","slug":"idea","permalink":"https://comwork2016.github.io/tags/idea/"},{"name":"license","slug":"license","permalink":"https://comwork2016.github.io/tags/license/"}]},{"title":"Python split保留分隔符","slug":"Python-split保留分隔符","date":"2017-03-31T08:21:37.000Z","updated":"2017-04-01T11:56:30.614Z","comments":true,"path":"2017/03/31/Python-split保留分隔符/","link":"","permalink":"https://comwork2016.github.io/2017/03/31/Python-split保留分隔符/","excerpt":"python 文本或句子切割，并保留分隔符","text":"python 文本或句子切割，并保留分隔符 主要思想，利用正则表达式re.split() 分割，同时利用re.findall() 查找分隔符，而后将二者链接即可。 12345678910111213141516171819202122# coding: utf-8import sysreload(sys)sys.setdefaultencoding(\"utf-8\")import redef my_split(str,sep=u\"要求\\d+|岗位\\S+\"): # 分隔符可为多样的正则表达式 wlist = re.split(sep,str) sepword = re.findall(sep,str) sepword.insert(0,\" \") # 开头（或末尾）插入一个空字符串，以保持长度和切割成分相同 wlist = [ x+y for x,y in zip(wlist,sepword) ] # 顺序可根据需求调换 return wlistif __name__ == \"__main__\": inputstr = \"岗位：学生： \\n要求1.必须好好学习。\\n要求2.必须踏实努力。\\n要求3.必须求实上进。\" res = my_split(inputstr) print '\\n'.join(res)","raw":null,"content":null,"categories":[{"name":"python","slug":"python","permalink":"https://comwork2016.github.io/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://comwork2016.github.io/tags/python/"},{"name":"split","slug":"split","permalink":"https://comwork2016.github.io/tags/split/"}]},{"title":"Python日志模块","slug":"Python日志模块","date":"2017-03-31T07:26:28.000Z","updated":"2017-04-01T11:56:30.614Z","comments":true,"path":"2017/03/31/Python日志模块/","link":"","permalink":"https://comwork2016.github.io/2017/03/31/Python日志模块/","excerpt":"python的标准库里的日志系统从Python2.3开始支持。只要import logging这个模块即可使用。","text":"python的标准库里的日志系统从Python2.3开始支持。只要import logging这个模块即可使用。 日志级别下表中的日志级别从上往下以此升高。logging只会输出比设定级别高的日志信息。 级别 何时使用 DEBUG 详细信息，典型地调试问题时会感兴趣。 INFO 证明事情按预期工作。 WARNING 表明发生了一些意外，或者不久的将来会发生问题（如‘磁盘满了’）。软件还是在正常工作。 ERROR 由于更严重的问题，软件已不能执行一些功能了。 CRITICAL 严重错误，表明软件已不能继续运行了。 关键概念Logger，Handler，Formatter和Filter是日志模块的几个基本概念，日志模块的工作原理要从这四个基本概念说起。 Logger 即记录器，Logger提供了日志相关功能的调用接口。 Handler 即处理器，将（记录器产生的）日志记录发送至合适的目的地。 Filter 即过滤器，提供了更好的粒度控制，它可以决定输出哪些日志记录。 Formatter 即格式化器，指明了最终输出中日志记录的格式。 LoggerLogger 即“记录器”，Logger对象实例是日志记录功能的载体，如： 1234567891011#!/usr/local/bin/python# -*- coding: utf-8 -*-import logginglogger = logging.getLogger('simple_example')logger.debug('debug message')logger.info('info message')logger.warn('warn message')logger.error('error message')logger.critical('critical message') 值得一提的是，Logger对象从不直接实例化，而是通过模块级的功能logging.getLogger(name)创建Logger实例。调用 logging.getLogger(name) 功能时，如果传入的name参数值相同，则总是返回同一个Logger对象实例的引用。 如果没有显式的进行创建，则默认创建一个root logger，并应用默认的日志级别(WARN)、默认的处理器Handler(StreamHandler，即将日志信息打印输出在标准输出上)，和默认的格式化器Formatter(默认的格式即为第一个简单使用程序中输出的格式)。 HandlerHandler 将日志信息发送到设置的位置，可以通过Logger对象的addHandler()方法为Logger对象添加0个或多个handler。一种日志的典型应用场景为，系统希望将所有的日志信息保存到log文件中，其中日志等级等于或高于ERROR的消息还要在屏幕标准输出上显示，日志等级为CRITICAL的还需要发送邮件通知；这种场景就需要3个独立的handler来实现需求，这三个handler分别与指定的日志等级或日志位置做响应 需要一提的是，为Logger配置的handler不能是Handler基类对象，而是Handler的子类对象，常用的Handler为StreamHandler, FileHandler和NullHandler。 FormatterFormatter 用于设置日志输出的格式，与前两个基本概念不同的是，该类可以直接初始化对象，即 formatter=logging.Formatter(fmt=None, datefmt=None)，创建formatter时，传入分别fmt和datefmt参数来修改日志格式和时间格式，默认的日志格式为%(asctime)s - %(levelname)s - %(message)s，默认的时间格式为%Y-%m-%d %H:%M:%S FilterFilter可用于Logger对象或Handler对象，用于提供比日志等级更加复杂的日志过滤方式。默认的filter只允许在指定logger层级下的日志消息通过过滤。例如，如果把filter设置为filter=logging.Filter(&#39;A.B&#39;)，则‘A.B’, ‘A.B.C’, ‘A.B.C.D’, ‘A.B.D’ 产生的日志信息可以通过过滤，但&#39;A.BB&#39;, &#39;B.A.B&#39;均不行。如果以空字符串初始化filter，则所有的日志消息都可以通过过滤。 Filter在日志功能配置中是非必须的，只在对日志消息过滤需求比较复杂时配置使用即可。 使用日志模块如果只需要在控制台输出日志信息，可以用如下格式： 1234import logginglogging.basicConfig(level=logging.DEBUG,format='%(levelname)s: %(asctime)s - %(filename)s:%(lineno)s - %(message)s')logging.info('info logging') 日志也可以同时在控制台和文件中输出，例如如下： 12345678910111213141516171819202122232425import logging# 创建一个loggerlogger = logging.getLogger('mylogger')logger.setLevel(logging.DEBUG)# 创建一个handler，用于写入日志文件fh = logging.FileHandler('test.log')fh.setLevel(logging.DEBUG)# 再创建一个handler，用于输出到控制台ch = logging.StreamHandler()ch.setLevel(logging.DEBUG)# 定义handler的输出格式formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')fh.setFormatter(formatter)ch.setFormatter(formatter)# 给logger添加handlerlogger.addHandler(fh)logger.addHandler(ch)# 记录一条日志logger.info('foorbar')","raw":null,"content":null,"categories":[{"name":"python","slug":"python","permalink":"https://comwork2016.github.io/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://comwork2016.github.io/tags/python/"},{"name":"日志","slug":"日志","permalink":"https://comwork2016.github.io/tags/日志/"}]},{"title":"搭建Hexo博客","slug":"搭建Hexo博客","date":"2017-03-31T02:45:23.000Z","updated":"2017-04-01T11:56:30.616Z","comments":true,"path":"2017/03/31/搭建Hexo博客/","link":"","permalink":"https://comwork2016.github.io/2017/03/31/搭建Hexo博客/","excerpt":"本文主要记录Windows系统下搭建Hexo博客，以防遗忘！","text":"本文主要记录Windows系统下搭建Hexo博客，以防遗忘！ 准备你需要准备好以下软件： Node.js Git 安装Hexo在自己认为合适的地方创建一个文件夹，然后在文件夹空白处按住Shift+鼠标右键，然后点击在此处打开命令行窗口。（同样要记住啦，下文中会使用在当前目录打开命令行来代指上述的操作） 在输入以下命令安装Hexo： 12345$ npm install hexo-cli -g$ hexo init$ npm install$ hexo g # 或者hexo generate$ hexo s # 或者hexo server，可以在http://localhost:4000/ 查看 这里有必要提下Hexo常用的几个命令： 12345$ hexo generate (hexo g) 生成静态文件，会在当前目录下生成一个新的叫做public的文件夹$ hexo server (hexo s) 启动本地web服务，用于博客的预览$ hexo deploy (hexo d) 部署播客到远端（比如github, heroku等平台）$ hexo new \"postName\" #新建文章$ hexo new page \"pageName\" #新建页面 常用简写1234$ hexo n == hexo new$ hexo g == hexo generate$ hexo s == hexo server$ hexo d == hexo deploy 在浏览器中打开http://localhost:4000/，你将会看到： 到目前为止，Hexo在本地的配置已经全都结束了。 修改全局配置文件具体可以参考Hexo官方文档 您可以在 _config.yml 中修改大部份的配置。 例如配置文件如下： 更换主题可以在此处寻找自己喜欢的主题下载所有的主题文件，保存到Hexo目录下的themes文件夹下。然后在_config.yml文件中修改： 1234# Extensions## Plugins: http://hexo.io/plugins/## Themes: http://hexo.io/themes/theme: landscape //themes文件夹中对应文件夹的名称 然后先执行hexo clean，然后重新hexo g，并且hexo d，很快就能看到新主题的效果了~ 创建代码库创建GitHub Pages的Repository name必须使用yourname.github.io，如图所示： 开启GitHub Pages功能开启GitHub Pages之前，必须有master分支存在。点击界面右侧的Settings，你将会打开这个库的setting页面，将GitHub Pages中的Source选中master： Github将会自动替你创建出一个gh-pages的页面。如果你的配置没有问题，yourname.github.io这个网址就可以正常访问了~如果yourname.github.io已经可以正常访问了，那么Github一侧的配置已经全部结束了。 部署Hexo到Github Pages这一步恐怕是最关键的一步了，让我们把在本地web环境下预览到的博客部署到github上，然后就可以直接通过http://yourname.github.io/访问了。 首先需要明白所谓部署到github的原理。 之前步骤中在Github上创建的那个特别的repo（yourname.github.io）一个最大的特点就是其master中的html静态文件，可以通过链接http://yourname.github.io/来直接访问。 Hexo -g会生成一个静态网站（第一次会生成一个public目录），这个静态文件可以直接访问。 需要将hexo生成的静态网站，提交(git commit)到github上。 明白了原理，怎么做自然就清晰了。 使用hexo deploy部署hexo deploy可以部署到很多平台，具体可以参考这个链接. 如果部署到github，需要在配置文件_config.xml中作如下修改： 1234deploy: type: git repo: git@github.com:comwork2016/comwork2016.github.io.git branch: master 然后在命令行中执行 1hexo d 即可完成部署。 踩坑提醒 注意需要提前安装一个扩展： 1$ npm install hexo-deployer-git --save 使用git命令行部署（optional）不幸的是，上述命令虽然简单方便，但是偶尔会有莫名其妙的问题出现，因此，我们也可以追本溯源，使用git命令来完成部署的工作。 clone github repo 12$ cd /blog$ git clone git@github.com:comwork2016/comwork2016.github.io.git .deploy/comwork2016.github.io 将我们之前创建的repo克隆到本地，新建一个目录叫做.deploy用于存放克隆的代码。 创建一个deploy脚本文件123456hexo generatecp -R public/* .deploy/comwork2016.github.iocd .deploy/comwork2016.github.iogit add .git commit -m updategit push origin master 简单解释一下，hexo generate生成public文件夹下的新内容，username.github.io的git目录下，然后使用git commit命令提交代码到username.github.io这个repo的master branch上。 需要部署的时候，执行这段脚本就可以了（比如可以将其保存为deploy.sh）。执行过程中可能需要让你输入Github账户的用户名及密码，按照提示操作即可。 更换域名首先，需要注册一个域名。在你的域名注册提供商那里配置DNS解析，获取GitHub的IP地址，进入source目录下，添加CNAME文件 12345$ cd source/$ touch CNAME$ vim CNAME # 输入你的域名$ git add CNAME$ git commit -m \"add CNAME\" 添加404公益页面GitHub Pages有提供制作404页面的指引：Custom 404 Pages。 直接在根目录下创建自己的404.html或者404.md就可以。但是自定义404页面仅对绑定顶级域名的项目才起作用，GitHub默认分配的二级域名是不起作用的，使用hexo server在本机调试也是不起作用的。 推荐使用腾讯公益404。 添加about页面1$ hexo new page \"about\" 之后在/source/about/index.md目录下会生成一个index.md文件，打开输入个人信息即可，如果想要添加版权信息，可以在文件末尾添加： 添加Fork me on Github获取代码，选择你喜欢的代码添加到/themes/yilia/layout/layout.ejs的末尾即可，注意要将代码里的you改成你的Github账号名。 添加支付宝捐赠按钮及二维码支付 支付宝捐赠按钮 在/themes/yilia/layout_widget目录下新建一个zhifubao.ejs文件，内容如下1234567891011&lt;p class=\"asidetitle\"&gt;打赏他&lt;/p&gt;&lt;div&gt;&lt;form action=\"https://shenghuo.alipay.com/send/payment/fill.htm\" method=\"POST\" target=\"_blank\" accept-charset=\"GBK\"&gt; &lt;br/&gt; &lt;input name=\"optEmail\" type=\"hidden\" value=\"your 支付宝账号\" /&gt; &lt;input name=\"payAmount\" type=\"hidden\" value=\"默认捐赠金额(元)\" /&gt; &lt;input id=\"title\" name=\"title\" type=\"hidden\" value=\"博主，打赏你的！\" /&gt; &lt;input name=\"memo\" type=\"hidden\" value=\"你Y加油，继续写博客！\" /&gt; &lt;input name=\"pay\" type=\"image\" value=\"转账\" src=\"http://7xig3q.com1.z0.glb.clouddn.com/alipay-donate-website.png\" /&gt;&lt;/form&gt;&lt;/div&gt; 添加完该文件之后，要在/themes/yilia/_config.ym文件中启用，如下所示，添加zhifubao1234567widgets:- category- tag- links- tagcloud- zhifubao- rss 二维码捐赠 首先需要到这里获取你的支付宝账户的二维码图片，支付宝提供了自定义功能，可以添加自定义文字。 我的二维码扫描捐赠添加在about页面，当然你也可以添加到其它页面，在\\source\\about下有index.md，打开，在适当位置添加12345&lt;center&gt;欢迎您捐赠本站，您的支持是我最大的动力！![][http://7xsxyo.com1.z0.glb.clouddn.com/2016/04/15/FoJ1F6Ht0CNaYuCdE2l52F-Fk9Vk202.png]&lt;/center&gt;&lt;br/&gt; 可以让图片居中显示，注意将图片链接地址换成你的即可。 本文原始链接：手把手教你使用Hexo + Github Pages搭建个人独立博客作者：令狐葱","raw":null,"content":null,"categories":[{"name":"tools","slug":"tools","permalink":"https://comwork2016.github.io/categories/tools/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://comwork2016.github.io/tags/hexo/"},{"name":"blog","slug":"blog","permalink":"https://comwork2016.github.io/tags/blog/"}]}]}